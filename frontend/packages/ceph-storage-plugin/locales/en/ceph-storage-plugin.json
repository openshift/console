{
  "Troubleshoot": "Troubleshoot",
  "Add Capacity": "Add Capacity",
  "Object Buckets": "Object Buckets",
  "Object Bucket Claims": "Object Bucket Claims",
  "Overview": "Overview",
  "Use existing claim": "Use existing claim",
  "Select claim": "Select claim",
  "Create new claim": "Create new claim",
  "Create": "Create",
  "Cancel": "Cancel",
  "Enabled": "Enabled",
  "Disabled": "Disabled",
  "Last synced": "Last synced",
  "BlockPool List": "BlockPool List",
  "Edit BlockPool": "Edit BlockPool",
  "Delete BlockPool": "Delete BlockPool",
  "{{replica}} Replication": "{{replica}} Replication",
  "Pool name": "Pool name",
  "my-block-pool": "my-block-pool",
  "pool-name-help": "pool-name-help",
  "Data protection policy": "Data protection policy",
  "Select replication": "Select replication",
  "Volume type": "Volume type",
  "Select volume type": "Select volume type",
  "Compression": "Compression",
  "Enable compression": "Enable compression",
  "Enabling compression may result in little or no space savings for encrypted or random data. Also, enabling compression may have an impact on I/O performance.": "Enabling compression may result in little or no space savings for encrypted or random data. Also, enabling compression may have an impact on I/O performance.",
  "OpenShift Container Storage's StorageCluster is not available. Try again after the StorageCluster is ready to use.": "OpenShift Container Storage's StorageCluster is not available. Try again after the StorageCluster is ready to use.",
  "Create BlockPool": "Create BlockPool",
  "Close": "Close",
  "Pool creation is not supported for OpenShift Data Foundation's external RHCS StorageSystem.": "Pool creation is not supported for OpenShift Data Foundation's external RHCS StorageSystem.",
  "A BlockPool is a logical entity providing elastic capacity to applications and workloads. Pools provide a means of supporting policies for access data resilience and storage efficiency.": "A BlockPool is a logical entity providing elastic capacity to applications and workloads. Pools provide a means of supporting policies for access data resilience and storage efficiency.",
  "BlockPool Creation Form": "BlockPool Creation Form",
  "Name": "Name",
  "Bucket Name": "Bucket Name",
  "Type": "Type",
  "Region": "Region",
  "BackingStore Table": "BackingStore Table",
  "Each BackingStore can be used for one tier at a time. Selecting a BackingStore in one tier will remove the resource from the second tier option and vice versa.": "Each BackingStore can be used for one tier at a time. Selecting a BackingStore in one tier will remove the resource from the second tier option and vice versa.",
  "Bucket created for OpenShift Data Foundation's Service": "Bucket created for OpenShift Data Foundation's Service",
  "Tier 1 - BackingStores": "Tier 1 - BackingStores",
  "Create BackingStore ": "Create BackingStore ",
  "Tier-1-Table": "Tier-1-Table",
  "{{bs, number}} BackingStore ": "{{bs, number}} BackingStore ",
  "{{bs, number}} BackingStore _plural": "{{bs, number}} BackingStore s",
  "selected": "selected",
  "Tier 2 - BackingStores": "Tier 2 - BackingStores",
  "Tier-2-Table": "Tier-2-Table",
  "General": "General",
  "Placement Policy": "Placement Policy",
  "Resources": "Resources",
  "Review": "Review",
  "Create BucketClass": "Create BucketClass",
  "Create new BucketClass": "Create new BucketClass",
  "BucketClass is a CRD representing a class for buckets that defines tiering policies and data placements for an OBC.": "BucketClass is a CRD representing a class for buckets that defines tiering policies and data placements for an OBC.",
  "Next": "Next",
  "Back": "Back",
  "Edit BucketClass Resource": "Edit BucketClass Resource",
  "{{storeType}} represents a storage target to be used as the underlying storage for the data in Multicloud Object Gateway buckets.": "{{storeType}} represents a storage target to be used as the underlying storage for the data in Multicloud Object Gateway buckets.",
  "Cancel ": "Cancel ",
  "Save": "Save",
  "What is a BackingStore?": "What is a BackingStore?",
  "BackingStore represents a storage target to be used as the underlying storage for the data in Multicloud Object Gateway buckets.": "BackingStore represents a storage target to be used as the underlying storage for the data in Multicloud Object Gateway buckets.",
  "Multiple types of BackingStores are supported: asws-s3 s3-compatible google-cloud-storage azure-blob obc PVC.": "Multiple types of BackingStores are supported: asws-s3 s3-compatible google-cloud-storage azure-blob obc PVC.",
  "Learn More": "Learn More",
  "What is a BucketClass?": "What is a BucketClass?",
  "A set of policies which would apply to all buckets (OBCs) created with the specific bucket class. These policies include placement, namespace and caching": "A set of policies which would apply to all buckets (OBCs) created with the specific bucket class. These policies include placement, namespace and caching",
  "BucketClass type": "BucketClass type",
  "3-63 chars": "3-63 chars",
  "Starts and ends with lowercase number or letter": "Starts and ends with lowercase number or letter",
  "Only lowercase letters, numbers, non-consecutive periods or hyphens": "Only lowercase letters, numbers, non-consecutive periods or hyphens",
  "Avoid using the form of an IP address": "Avoid using the form of an IP address",
  "Globally unique name": "Globally unique name",
  "BucketClass name": "BucketClass name",
  "A unique name for the bucket class within the project.": "A unique name for the bucket class within the project.",
  "my-multi-cloud-mirror": "my-multi-cloud-mirror",
  "BucketClass Name": "BucketClass Name",
  "Description (Optional)": "Description (Optional)",
  "Description of bucket class": "Description of bucket class",
  "What is a Namespace Policy?": "What is a Namespace Policy?",
  "Namespace policy can be set to one single read and write source, multi read sources or cached policy.": "Namespace policy can be set to one single read and write source, multi read sources or cached policy.",
  "Namespace Policy Type": "Namespace Policy Type",
  "What is Caching?": "What is Caching?",
  "Caching is a policy that creates local copies of the data. It saves the copies locally to improve performance for frequently accessed data. Each cached copy has a TTL and is verified against the hub. Each non-read operation (upload, overwrite, delete) is performed on the hub": "Caching is a policy that creates local copies of the data. It saves the copies locally to improve performance for frequently accessed data. Each cached copy has a TTL and is verified against the hub. Each non-read operation (upload, overwrite, delete) is performed on the hub",
  "Hub namespace store ": "Hub namespace store ",
  "A single NamespaceStore that defines the read and write target of the namespace bucket.": "A single NamespaceStore that defines the read and write target of the namespace bucket.",
  "NamespaceStore": "NamespaceStore",
  "Cache data settings": "Cache data settings",
  "The data will be temporarily copied on a backing store in order to later access it much more quickly.": "The data will be temporarily copied on a backing store in order to later access it much more quickly.",
  "Backing store": "Backing store",
  "a local backing store is recommended for better performance": "a local backing store is recommended for better performance",
  "Time to live": "Time to live",
  "Time to live is the time that an object is stored in a caching system before it is deleted or refreshed. Default: 1 hr, Min: 15 mins, Max: 24 hrs": "Time to live is the time that an object is stored in a caching system before it is deleted or refreshed. Default: 1 hr, Min: 15 mins, Max: 24 hrs",
  "Read NamespaceStores": "Read NamespaceStores",
  "Select a list of NamespaceStores that defines the read targets of the namespace bucket.": "Select a list of NamespaceStores that defines the read targets of the namespace bucket.",
  "Create NamespaceStore": "Create NamespaceStore",
  "{{nns, number}} namespace store ": "{{nns, number}} namespace store",
  "{{nns, number}} namespace store _plural": "{{nns, number}} namespace stores",
  " selected": " selected",
  "Write NamespaceStore": "Write NamespaceStore",
  "Select a single NamespaceStore that defines the write targets of the namespace bucket.": "Select a single NamespaceStore that defines the write targets of the namespace bucket.",
  "Read and Write NamespaceStore ": "Read and Write NamespaceStore ",
  "Select one NamespaceStore which defines the read and write targets of the namespace bucket.": "Select one NamespaceStore which defines the read and write targets of the namespace bucket.",
  "What is a Placement Policy?": "What is a Placement Policy?",
  "Data placement capabilities are built as a multi-layer structure here are the layers bottom-up:": "Data placement capabilities are built as a multi-layer structure here are the layers bottom-up:",
  "Spread Tier - list of BackingStores aggregates the storage of multiple stores.": "Spread Tier - list of BackingStores aggregates the storage of multiple stores.",
  "Mirroring Tier - list of spread-layers async-mirroring to all mirrors with locality optimization (will allocate on the closest region to the source endpoint). Mirroring requires at least two BackingStores.": "Mirroring Tier - list of spread-layers async-mirroring to all mirrors with locality optimization (will allocate on the closest region to the source endpoint). Mirroring requires at least two BackingStores.",
  "The number of replicas can be configured via the NooBaa management console.": "The number of replicas can be configured via the NooBaa management console.",
  "Tier 1 - Policy Type": "Tier 1 - Policy Type",
  "Spread": "Spread",
  "Spreading the data across the chosen resources. By default a replica of one copy is used and does not include failure tolerance in case of resource failure.": "Spreading the data across the chosen resources. By default a replica of one copy is used and does not include failure tolerance in case of resource failure.",
  "Mirror": "Mirror",
  "Full duplication of the data in each chosen resource. By default a replica of one copy per location is used. Includes failure tolerance in case of resource failure.": "Full duplication of the data in each chosen resource. By default a replica of one copy per location is used. Includes failure tolerance in case of resource failure.",
  "Add Tier": "Add Tier",
  "Tier 2 - Policy type": "Tier 2 - Policy type",
  "Remove Tier": "Remove Tier",
  "Spreading the data across the chosen resources does not include failure tolerance in case of resource failure.": "Spreading the data across the chosen resources does not include failure tolerance in case of resource failure.",
  "Full duplication of the data in each chosen resource includes failure tolerance in cause of resource failure.": "Full duplication of the data in each chosen resource includes failure tolerance in cause of resource failure.",
  "Namespace Policy: ": "Namespace Policy: ",
  "Read and write NamespaceStore : ": "Read and write NamespaceStore : ",
  "Hub namespace store: ": "Hub namespace store: ",
  "Cache backing store: ": "Cache backing store: ",
  "Time to live: ": "Time to live: ",
  "Resources ": "Resources ",
  "Selected read namespace stores: ": "Selected read namespace stores: ",
  "Selected write namespace store: ": "Selected write namespace store: ",
  "Placement policy details ": "Placement policy details ",
  "Tier 1: ": "Tier 1: ",
  "Selected BackingStores": "Selected BackingStores",
  "Tier 2: ": "Tier 2: ",
  "Review BucketClass": "Review BucketClass",
  "BucketClass type: ": "BucketClass type: ",
  "BucketClass name: ": "BucketClass name: ",
  "Description: ": "Description: ",
  "Provider {{provider}}": "Provider {{provider}}",
  "Create new BackingStore ": "Create new BackingStore ",
  "An error has occured while fetching backing stores": "An error has occured while fetching backing stores",
  "Select a backing store": "Select a backing store",
  "Storage targets that are used to store chunks of data on Multicloud Object Gateway buckets.": "Storage targets that are used to store chunks of data on Multicloud Object Gateway buckets.",
  "A BackingStore represents a storage target to be used as the underlying storage layer in Multicloud Object Gateway buckets.": "A BackingStore  represents a storage target to be used as the underlying storage layer in Multicloud Object Gateway buckets.",
  "Multiple types of BackingStores are supported: AWS S3 S3 Compatible Google Cloud Storage Azure Blob PVC.": "Multiple types of BackingStores are supported: AWS S3 S3 Compatible Google Cloud Storage Azure Blob PVC.",
  "BackingStore Name": "BackingStore Name",
  "A unique name for the BackingStore  within the project": "A unique name for the BackingStore  within the project",
  "Name can contain a max of 43 characters": "Name can contain a max of 43 characters",
  "Provider": "Provider",
  "Create BackingStore": "Create BackingStore",
  "This is an Advanced subscription feature. It requires Advanced Edition subscription. Please contact the account team for more information.": "This is an Advanced subscription feature. It requires Advanced Edition subscription. Please contact the account team for more information.",
  "Advanced Subscription": "Advanced Subscription",
  "Advanced": "Advanced",
  "Deployment type": "Deployment type",
  "Multi cloud object gateway": "Multi cloud object gateway",
  "Object storage": "Object storage",
  "Select external system from list": "Select external system from list",
  "Use an existing storage class": "Use an existing storage class",
  "Can be used on all platforms except BareMetal. OpenShift Data Foundation will use an infrastructure storage class provided by the hosting platform.": "Can be used on all platforms except BareMetal. OpenShift Data Foundation will use an infrastructure storage class provided by the hosting platform.",
  "Create a new storage class using local devices": "Create a new storage class using local devices",
  "Can be used on any platform having nodes with local devices. The infrastructure storage class is provided by Local Storage Operator on top of the local devices.": "Can be used on any platform having nodes with local devices. The infrastructure storage class is provided by Local Storage Operator on top of the local devices.",
  "Connect a new external storage system": "Connect a new external storage system",
  "Can be used to connect an external storage platform to OpenShift Data Foundation.": "Can be used to connect an external storage platform to OpenShift Data Foundation.",
  "{{nodeCount, number}} node": "{{nodeCount, number}} node",
  "{{nodeCount, number}} node_plural": "{{nodeCount, number}} nodes",
  "selected ({{cpu}} CPU and {{memory}} on ": "selected ({{cpu}} CPU and {{memory}} on ",
  "{{zoneCount, number}} zone": "{{zoneCount, number}} zone",
  "{{zoneCount, number}} zone_plural": "{{zoneCount, number}} zones",
  "Select Capacity": "Select Capacity",
  "Requested Capacity": "Requested Capacity",
  "Select Nodes": "Select Nodes",
  "Select at least 3 nodes preferably in 3 different zones. It is recommended to start with at least 14 CPUs and 34 GiB per node.": "Select at least 3 nodes preferably in 3 different zones. It is recommended to start with at least 14 CPUs and 34 GiB per node.",
  "Search by node name...": "Search by node name...",
  "Search by node label...": "Search by node label...",
  "Available Raw Capacity": "Available Raw Capacity",
  "The available capacity is based on all attached disks associated with the selectedStorageClass <2>\"{{storageClassName}}\"</2>": "The available capacity is based on all attached disks associated with the selectedStorageClass <2>\"{{storageClassName}}\"</2>",
  "Selected Nodes": "Selected Nodes",
  "Role": "Role",
  "CPU": "CPU",
  "Memory": "Memory",
  "Zone": "Zone",
  "Selected nodes table": "Selected nodes table",
  "Connection details": "Connection details",
  "LocalVolumeSet Name": "LocalVolumeSet Name",
  "StorageClass Name": "StorageClass Name",
  "Filter Disks By": "Filter Disks By",
  "Disks on all nodes": "Disks on all nodes",
  "{{nodes, number}} node": "{{nodes, number}} node",
  "{{nodes, number}} node_plural": "{{nodes, number}} nodes",
  "Disks on selected nodes": "Disks on selected nodes",
  "Uses the available disks that match the selected filters only on selected nodes.": "Uses the available disks that match the selected filters only on selected nodes.",
  "Disk Type": "Disk Type",
  "Volume Mode": "Volume Mode",
  "Disk Size": "Disk Size",
  "Min": "Min",
  "Max": "Max",
  "Maximum Disks Limit": "Maximum Disks Limit",
  "Disks limit will set the maximum number of PVs to create on a node. If the field is empty we will create PVs for all available disks on the matching nodes.": "Disks limit will set the maximum number of PVs to create on a node. If the field is empty we will create PVs for all available disks on the matching nodes.",
  "All": "All",
  "Uses the available disks that match the selected filters on all nodes selected in the previous step.": "Uses the available disks that match the selected filters on all nodes selected in the previous step.",
  "A LocalVolumeSet allows you to filter a set of disks, group them and create a dedicated StorageClass to consume storage from them.": "A LocalVolumeSet allows you to filter a set of disks, group them and create a dedicated StorageClass to consume storage from them.",
  "Local Storage Operator not installed": "Local Storage Operator not installed",
  "Before we can create a StorageCluster, the Local Storage operator needs to be installed. When installation is finished come back to OpenShift Container Storage to create a StorageCluster.<1><0>Install</0></1>": "Before we can create a StorageCluster, the Local Storage operator needs to be installed. When installation is finished come back to OpenShift Container Storage to create a StorageCluster.<1><0>Install</0></1>",
  "Minimum Node Requirement": "Minimum Node Requirement",
  "A minimum of 3 nodes are required for the initial deployment. Only {{nodes}} node match to the selected filters. Please adjust the filters to include more nodes.": "A minimum of 3 nodes are required for the initial deployment. Only {{nodes}} node match to the selected filters. Please adjust the filters to include more nodes.",
  "After the LocalVolumeSet and StorageClass are created you won't be able to go back to this step.": "After the LocalVolumeSet and StorageClass are created you won't be able to go back to this step.",
  "Note:": "Note:",
  "Create StorageClass": "Create StorageClass",
  "Yes": "Yes",
  "Are you sure you want to continue?": "Are you sure you want to continue?",
  "Node": "Node",
  "Model": "Model",
  "Capacity": "Capacity",
  "Selected Disks": "Selected Disks",
  "Disk List": "Disk List",
  "Selected Capacity": "Selected Capacity",
  "{{nodes, number}} Node": "{{nodes, number}} Node",
  "{{nodes, number}} Node_plural": "{{nodes, number}} Nodes",
  "{{disks, number}} Disk": "{{disks, number}} Disk",
  "{{disks, number}} Disk_plural": "{{disks, number}} Disks",
  "Selected versus Available Capacity": "Selected versus Available Capacity",
  "Out of {{capacity}}": "Out of {{capacity}}",
  "StorageClass name": "StorageClass name",
  "Backing storage": "Backing storage",
  "StorageClass:": "StorageClass:",
  "Capacity and nodes": "Capacity and nodes",
  "Requested Cluster Capacity:": "Requested Cluster Capacity:",
  "Total CPU and memory of {{cpu, number}} CPU and {{memory}}": "Total CPU and memory of {{cpu, number}} CPU and {{memory}}",
  "Configure": "Configure",
  "Enable Encryption": "Enable Encryption",
  "Connect to external key management service: {{name}}": "Connect to external key management service: {{name}}",
  "Encryption Level: {{level}}": "Encryption Level: {{level}}",
  "Using {{networkLabel}}": "Using {{networkLabel}}",
  "StorageClass encryption": "StorageClass encryption",
  "The StorageCluster encryption level can be set to include all components under the cluster (including StorageClass and PVs) or to include only StorageClass encryption. PV encryption can use an auth token that will be used with the KMS configuration to allow multi-tenancy.": "The StorageCluster encryption level can be set to include all components under the cluster (including StorageClass and PVs) or to include only StorageClass encryption. PV encryption can use an auth token that will be used with the KMS configuration to allow multi-tenancy.",
  "Data encryption for block and file storage. MultiCloud Object Gateway is always encrypted.": "Data encryption for block and file storage. MultiCloud Object Gateway is always encrypted.",
  "Encryption level": "Encryption level",
  "Cluster-wide encryption": "Cluster-wide encryption",
  "Encryption for the entire cluster (block and file)": "Encryption for the entire cluster (block and file)",
  "An encryption key will be generated for each persistent volume (block) created using an encryption enabled StorageClass.": "An encryption key will be generated for each persistent volume (block) created using an encryption enabled StorageClass.",
  "Connection settings": "Connection settings",
  "Connect to an external key management service": "Connect to an external key management service",
  "An error has occurred: {{error}}": "An error has occurred: {{error}}",
  "Endpoint": "Endpoint",
  "Rest API IP address of IBM FlashSystem.": "Rest API IP address of IBM FlashSystem.",
  "The endpoint is not a valid URL": "The endpoint is not a valid URL",
  "Username": "Username",
  "Password": "Password",
  "Hide password": "Hide password",
  "Reveal password": "Reveal password",
  "Poolname": "Poolname",
  "External storage metadata": "External storage metadata",
  "Download <1>{{SCRIPT_NAME}}</1> script and run on the RHCS cluster, then upload the results (JSON).": "Download <1>{{SCRIPT_NAME}}</1> script and run on the RHCS cluster, then upload the results (JSON).",
  "Download script": "Download script",
  "The uploaded file is not a valid JSON file": "The uploaded file is not a valid JSON file",
  "Browse": "Browse",
  "Upload JSON file": "Upload JSON file",
  "An error has occurred": "An error has occurred",
  "Create StorageSystem": "Create StorageSystem",
  "StorageSystem is an entity of OpenShift Data Foundation. It represents all of the required storage and compute resources.": "StorageSystem is an entity of OpenShift Data Foundation. It represents all of the required storage and compute resources.",
  "Not found": "Not found",
  "Details": "Details",
  "Replicas": "Replicas",
  "Inventory": "Inventory",
  "Not available": "Not available",
  "Image states info": "Image states info",
  "What does each state mean?": "What does each state mean?",
  " <1>Starting replay:</1> Initiating image (PV) replication process. ": " <1>Starting replay:</1> Initiating image (PV) replication process. ",
  " <1>Replaying:</1> Image (PV) replication is ongoing or idle between clusters. ": " <1>Replaying:</1> Image (PV) replication is ongoing or idle between clusters. ",
  " <1>Stopping replay:</1> Image (PV) replication process is shutting down. ": " <1>Stopping replay:</1> Image (PV) replication process is shutting down. ",
  " <1>Stopped:</1> Image (PV) replication process has shut down. ": " <1>Stopped:</1> Image (PV) replication process has shut down. ",
  " <1>Error:</1> Image (PV) replication process stopped due to an error. ": " <1>Error:</1> Image (PV) replication process stopped due to an error. ",
  " <1>Unknown:</1> Unable to determine image (PV) state due to an error. Check your network connection and remote cluster mirroring daemon. ": " <1>Unknown:</1> Unable to determine image (PV) state due to an error. Check your network connection and remote cluster mirroring daemon. ",
  "image states info": "image states info",
  "Image States": "Image States",
  "Mirroring": "Mirroring",
  "Mirroring status": "Mirroring status",
  "Overall image health": "Overall image health",
  "Show image states": "Show image states",
  "Last checked": "Last checked",
  "Raw Capacity shows the total physical capacity from all storage media within the storage subsystem": "Raw Capacity shows the total physical capacity from all storage media within the storage subsystem",
  "Start replay": "Start replay",
  "Stop reply": "Stop reply",
  "Replaying": "Replaying",
  "Stopped": "Stopped",
  "Error": "Error",
  "Syncing": "Syncing",
  "Unknown": "Unknown",
  "Status": "Status",
  "Performance": "Performance",
  "IOPS": "IOPS",
  "Throughput": "Throughput",
  "Not enough usage data": "Not enough usage data",
  "used": "used",
  "available": "available",
  "Other": "Other",
  "All other capacity usage that are not a part of the top 5 consumers.": "All other capacity usage that are not a part of the top 5 consumers.",
  "Available": "Available",
  "Warning": "Warning",
  "Raw capacity": "Raw capacity",
  "Used": "Used",
  "Available versus Used Capacity": "Available versus Used Capacity",
  "Used of {{capacity}}": "Used of {{capacity}}",
  "Not Available": "Not Available",
  "Rebuilding data resiliency": "Rebuilding data resiliency",
  "{{formattedProgress, number}}%": "{{formattedProgress, number}}%",
  "Activity": "Activity",
  "Estimating {{formattedEta}} to completion": "Estimating {{formattedEta}} to completion",
  "Object": "Object",
  "Object_plural": "Objects",
  "Buckets": "Buckets",
  "Buckets card represents the number of S3 buckets managed on Multicloud Object Gateway and the number of ObjectBucketClaims and the ObjectBuckets managed on both Multicloud Object Gateway and RGW (if deployed).": "Buckets card represents the number of S3 buckets managed on Multicloud Object Gateway and the number of ObjectBucketClaims and the ObjectBuckets managed on both Multicloud Object Gateway and RGW (if deployed).",
  "NooBaa Bucket": "NooBaa Bucket",
  "Break by": "Break by",
  "Total": "Total",
  "Projects": "Projects",
  "BucketClasses": "BucketClasses",
  "Service type": "Service type",
  "Cluster-wide": "Cluster-wide",
  "Any NON Object bucket claims that were created via an S3 client or via the NooBaa UI system.": "Any NON Object bucket claims that were created via an S3 client or via the NooBaa UI system.",
  "Capacity breakdown": "Capacity breakdown",
  "This card shows used capacity for different resources. The available capacity is based on cloud services therefore it cannot be shown.": "This card shows used capacity for different resources. The available capacity is based on cloud services therefore it cannot be shown.",
  "Type: {{serviceType}}": "Type: {{serviceType}}",
  "Service Type Dropdown": "Service Type Dropdown",
  "Service Type Dropdown Toggle": "Service Type Dropdown Toggle",
  "By: {{serviceType}}": "By: {{serviceType}}",
  "Break By Dropdown": "Break By Dropdown",
  "Providers": "Providers",
  "Accounts": "Accounts",
  "Metric": "Metric",
  "I/O Operations": "I/O Operations",
  "Logial Used Capacity": "Logial Used Capacity",
  "Physical vs. Logical used capacity": "Physical vs. Logical used capacity",
  "Egress": "Egress",
  "Latency": "Latency",
  "Bandwidth": "Bandwidth",
  "Service Type": "Service Type",
  "Type: {{selectedService}}": "Type: {{selectedService}}",
  "{{selectedMetric}} by {{selectedBreakdown}}": "{{selectedMetric}} by {{selectedBreakdown}}",
  "thousands": "thousands",
  "millions": "millions",
  "billions": "billions",
  "Total Reads {{totalRead}}": "Total Reads {{totalRead}}",
  "Total Writes {{totalWrite}}": "Total Writes {{totalWrite}}",
  "Total Logical Used Capacity {{logicalCapacity}}": "Total Logical Used Capacity {{logicalCapacity}}",
  "Total Physical Used Capacity {{physicalcapacity}}": "Total Physical Used Capacity {{physicalcapacity}}",
  "Shows an overview of the data consumption per provider or account collected from the day of the entity creation.": "Shows an overview of the data consumption per provider or account collected from the day of the entity creation.",
  "(in {{suffixLabel}})": "(in {{suffixLabel}})",
  "GET {{GETLatestValue}}": "GET {{GETLatestValue}}",
  "PUT {{PUTLatestValue}}": "PUT {{PUTLatestValue}}",
  "Service Name": "Service Name",
  "OpenShift Container Storage": "OpenShift Container Storage",
  "System Name": "System Name",
  "Multicloud Object Gateway": "Multicloud Object Gateway",
  "RADOS Object Gateway": "RADOS Object Gateway",
  "Version": "Version",
  "Resource Providers": "Resource Providers",
  "A list of all Multicloud Object Gateway resources that are currently in use. Those resources are used to store data according to the buckets' policies and can be a cloud-based resource or a bare metal resource.": "A list of all Multicloud Object Gateway resources that are currently in use. Those resources are used to store data according to the buckets' policies and can be a cloud-based resource or a bare metal resource.",
  "Object Service": "Object Service",
  "Data Resiliency": "Data Resiliency",
  "Object Service Status": "Object Service Status",
  "The object service includes 2 services.": "The object service includes 2 services.",
  "The data resiliency includes 2 services": "The data resiliency includes 2 services",
  "Services": "Services",
  "Object Gateway (RGW)": "Object Gateway (RGW)",
  "All resources are unhealthy": "All resources are unhealthy",
  "Object Bucket has an issue": "Object Bucket has an issue",
  "Many buckets have issues": "Many buckets have issues",
  "Some buckets have issues": "Some buckets have issues",
  "{{capacityRatio, number}}:1": "{{capacityRatio, number}}:1",
  "Compression ratio": "Compression ratio",
  "OpenShift Data Foundation can be configured to use compression. The efficiency rate reflects the actual compression ratio when using such a configuration.": "OpenShift Data Foundation can be configured to use compression. The efficiency rate reflects the actual compression ratio when using such a configuration.",
  "Savings": "Savings",
  "Savings shows the uncompressed and non-deduped data that would have been stored without those techniques.": "Savings shows the uncompressed and non-deduped data that would have been stored without those techniques.",
  "Storage Efficiency": "Storage Efficiency",
  "OpenShift Data Foundation Overview": "OpenShift Data Foundation Overview",
  "Storage Classes": "Storage Classes",
  "Pods": "Pods",
  "{{metricType}}": "{{metricType}}",
  "Break by dropdown": "Break by dropdown",
  "Cluster Name": "Cluster Name",
  "Mode": "Mode",
  "Storage Cluster": "Storage Cluster",
  "Utilization": "Utilization",
  "Used Capacity": "Used Capacity",
  "Requested capacity": "Requested capacity",
  "Expanding StorageCluster": "Expanding StorageCluster",
  "Upgrading OpenShift Data Foundation's Operator": "Upgrading OpenShift Data Foundation's Operator",
  "Used Capacity Breakdown": "Used Capacity Breakdown",
  "This card shows the used capacity for different Kubernetes resources. The figures shown represent the Usable storage, meaning that data replication is not taken into consideration.": "This card shows the used capacity for different Kubernetes resources. The figures shown represent the Usable storage, meaning that data replication is not taken into consideration.",
  "Service name": "Service name",
  "Cluster name": "Cluster name",
  "Internal": "Internal",
  "Raw capacity is the absolute total disk space available to the array subsystem.": "Raw capacity is the absolute total disk space available to the array subsystem.",
  "Active health checks": "Active health checks",
  "Progressing": "Progressing",
  "The Compression Ratio represents the compressible data effectiveness metric inclusive of all compression-enabled pools.": "The Compression Ratio represents the compressible data effectiveness metric inclusive of all compression-enabled pools.",
  "The Savings metric represents the actual disk capacity saved inclusive of all compression-enabled pools and associated replicas.": "The Savings metric represents the actual disk capacity saved inclusive of all compression-enabled pools and associated replicas.",
  "Performance metrics over time showing IOPS, Latency and more. Each metric is a link to a detailed view of this metric.": "Performance metrics over time showing IOPS, Latency and more. Each metric is a link to a detailed view of this metric.",
  "Recovery": "Recovery",
  "Disk State": "Disk State",
  "OpenShift Data Foundation status": "OpenShift Data Foundation status",
  "Filesystem": "Filesystem",
  "Disks List": "Disks List",
  "Start Disk Replacement": "Start Disk Replacement",
  "<0>{{diskName}}</0> can be replaced with a disk of same type.": "<0>{{diskName}}</0> can be replaced with a disk of same type.",
  "Troubleshoot disk <1>{{diskName}}</1>": "Troubleshoot disk <1>{{diskName}}</1>",
  "here": "here",
  "Online": "Online",
  "Offline": "Offline",
  "NotResponding": "NotResponding",
  "PreparingToReplace": "PreparingToReplace",
  "ReplacementFailed": "ReplacementFailed",
  "ReplacementReady": "ReplacementReady",
  "This is a required field": "This is a required field",
  "Please enter a URL": "Please enter a URL",
  "Please enter a valid port": "Please enter a valid port",
  "Key Management Service Provider": "Key Management Service Provider",
  "kms-provider-name": "kms-provider-name",
  "Address": "Address",
  "Port": "Port",
  "Token": "Token",
  "Advanced Settings": "Advanced Settings",
  "No StorageClass selected": "No StorageClass selected",
  "The Arbiter stretch cluster requires a minimum of 4 nodes (2 different zones, 2 nodes per zone). Please choose a different StorageClass or create a new LocalVolumeSet that matches the minimum node requirement.": "The Arbiter stretch cluster requires a minimum of 4 nodes (2 different zones, 2 nodes per zone). Please choose a different StorageClass or create a new LocalVolumeSet that matches the minimum node requirement.",
  "The StorageCluster requires a minimum of 3 nodes. Please choose a different StorageClass or create a new LocalVolumeSet that matches the minimum node requirement.": "The StorageCluster requires a minimum of 3 nodes. Please choose a different StorageClass or create a new LocalVolumeSet that matches the minimum node requirement.",
  "Adding capacity for <1>{{name}}</1>, may increase your expenses.": "Adding capacity for <1>{{name}}</1>, may increase your expenses.",
  "StorageClass": "StorageClass",
  "Raw Capacity": "Raw Capacity",
  "x {{ replica, number }} replicas =": "x {{ replica, number }} replicas =",
  "Currently Used:": "Currently Used:",
  "Add": "Add",
  "Vault enterprise namespaces are isolated environments that functionally exist as Vaults within a Vault. They have separate login paths and support creating and managing data isolated to their namespace.": "Vault enterprise namespaces are isolated environments that functionally exist as Vaults within a Vault. They have separate login paths and support creating and managing data isolated to their namespace.",
  "Maximum file size exceeded. File limit is 4MB.": "Maximum file size exceeded. File limit is 4MB.",
  "A PEM-encoded CA certificate file used to verify the Vault server's SSL certificate.": "A PEM-encoded CA certificate file used to verify the Vault server's SSL certificate.",
  "A PEM-encoded client certificate. This certificate is used for TLS communication with the Vault server.": "A PEM-encoded client certificate. This certificate is used for TLS communication with the Vault server.",
  "An unencrypted, PEM-encoded private key which corresponds to the matching client certificate provided with VAULT_CLIENT_CERT.": "An unencrypted, PEM-encoded private key which corresponds to the matching client certificate provided with VAULT_CLIENT_CERT.",
  "The name to use as the SNI host when OpenShift Data Foundation connecting via TLS to the Vault server": "The name to use as the SNI host when OpenShift Data Foundation connecting via TLS to the Vault server",
  "Key Management Service Advanced Settings": "Key Management Service Advanced Settings",
  "Backend Path": "Backend Path",
  "path/": "path/",
  "TLS Server Name": "TLS Server Name",
  "Vault Enterprise Namespace": "Vault Enterprise Namespace",
  "The name must be accurate and must match the service namespace": "The name must be accurate and must match the service namespace",
  "CA Certificate": "CA Certificate",
  "Upload a .PEM file here...": "Upload a .PEM file here...",
  "Client Certificate": "Client Certificate",
  "Client Private Key": "Client Private Key",
  "Attach OBC to a Deployment": "Attach OBC to a Deployment",
  "Deployment Name": "Deployment Name",
  "Attach": "Attach",
  "<0><0>{{poolName}}</0> cannot be deleted. When a pool is bounded to PVC it cannot be deleted. Please detach all the resources from StorageClass(es):</0>": "<0><0>{{poolName}}</0> cannot be deleted. When a pool is bounded to PVC it cannot be deleted. Please detach all the resources from StorageClass(es):</0>",
  "<0>Deleting <1>{{poolName}}</1> will remove all the saved data of this pool. Are you sure want to delete?</0>": "<0>Deleting <1>{{poolName}}</1> will remove all the saved data of this pool. Are you sure want to delete?</0>",
  "BlockPool Delete Modal": "BlockPool Delete Modal",
  "Try Again": "Try Again",
  "Finish": "Finish",
  "Go To Pvc List": "Go To Pvc List",
  "BlockPool Update Form": "BlockPool Update Form",
  "replacement disallowed: disk {{diskName}} is {{replacingDiskStatus}}": "replacement disallowed: disk {{diskName}} is {{replacingDiskStatus}}",
  "replacement disallowed: disk {{diskName}} is {{replacementStatus}}": "replacement disallowed: disk {{diskName}} is {{replacementStatus}}",
  "Disk Replacement": "Disk Replacement",
  "This action will start preparing the disk for replacement.": "This action will start preparing the disk for replacement.",
  "Data rebalancing is in progress": "Data rebalancing is in progress",
  "See data resiliency status": "See data resiliency status",
  "Are you sure you want to replace <1>{{diskName}}</1>?": "Are you sure you want to replace <1>{{diskName}}</1>?",
  "Replace": "Replace",
  "Create NamespaceStore ": "Create NamespaceStore ",
  "Represents an underlying storage to be used as read or write target for the data in the namespace buckets.": "Represents an underlying storage to be used as read or write target for the data in the namespace buckets.",
  "Provider {{provider}} | Region: {{region}}": "Provider {{provider}} | Region: {{region}}",
  "Create new NamespaceStore ": "Create new NamespaceStore ",
  "An error has occurred while fetching namespace stores": "An error has occurred while fetching namespace stores",
  "Select a namespace store": "Select a namespace store",
  "Namespace store name": "Namespace store name",
  "A unique name for the namespace store within the project": "A unique name for the namespace store within the project",
  "Namespace Store Table": "Namespace Store Table",
  "Where can I find Google Cloud credentials?": "Where can I find Google Cloud credentials?",
  "Service account keys are needed for Google Cloud Storage authentication. The keys can be found in the service accounts page in the GCP console.": "Service account keys are needed for Google Cloud Storage authentication. The keys can be found in the service accounts page in the GCP console.",
  "Learn more": "Learn more",
  "Upload a .json file with the service account keys provided by Google Cloud Storage.": "Upload a .json file with the service account keys provided by Google Cloud Storage.",
  "Secret Key": "Secret Key",
  "Upload JSON": "Upload JSON",
  "Uploaded File Name": "Uploaded File Name",
  "Upload File": "Upload File",
  "Switch to Secret": "Switch to Secret",
  "Select Secret": "Select Secret",
  "Switch to upload JSON": "Switch to upload JSON",
  "Cluster Metadata": "Cluster Metadata",
  "Target Bucket": "Target Bucket",
  "Number of Volumes": "Number of Volumes",
  "Volume Size": "Volume Size",
  "Target blob container": "Target blob container",
  "Target bucket": "Target bucket",
  "Account name": "Account name",
  "Access key": "Access key",
  "Account key": "Account key",
  "Secret key": "Secret key",
  "Region Dropdown": "Region Dropdown",
  "Endpoint Address": "Endpoint Address",
  "Secret": "Secret",
  "Switch to Credentials": "Switch to Credentials",
  "Access Key Field": "Access Key Field",
  "Secret Key Field": "Secret Key Field",
  "ObjectBucketClaim Name": "ObjectBucketClaim Name",
  "my-object-bucket": "my-object-bucket",
  "If not provided a generic name will be generated.": "If not provided a generic name will be generated.",
  "Defines the object-store service and the bucket provisioner.": "Defines the object-store service and the bucket provisioner.",
  "BucketClass": "BucketClass",
  "Select BucketClass": "Select BucketClass",
  "Create ObjectBucketClaim": "Create ObjectBucketClaim",
  "Edit YAML": "Edit YAML",
  "Attach to Deployment": "Attach to Deployment",
  "Object Bucket Claim Details": "Object Bucket Claim Details",
  "Object Bucket": "Object Bucket",
  "Namespace": "Namespace",
  "OBCTableHeader": "OBCTableHeader",
  "Object Bucket Claim Data": "Object Bucket Claim Data",
  "Hide Values": "Hide Values",
  "Reveal Values": "Reveal Values",
  "Data": "Data",
  "Create Object Bucket": "Create Object Bucket",
  "Object Bucket Name": "Object Bucket Name",
  "ob-name-help": "ob-name-help",
  "Object Bucket Details": "Object Bucket Details",
  "Object Bucket Claim": "Object Bucket Claim",
  "OBTableHeader": "OBTableHeader",
  "OpenShift Container Storage's StorageCluster requires a minimum of 3 nodes for the initial deployment. Only {{nodes}} node match to the selected filters. Please adjust the filters to include more nodes.": "OpenShift Container Storage's StorageCluster requires a minimum of 3 nodes for the initial deployment. Only {{nodes}} node match to the selected filters. Please adjust the filters to include more nodes.",
  "Review StorageCluster": "Review StorageCluster",
  "Storage and nodes": "Storage and nodes",
  "Arbiter zone:": "Arbiter zone:",
  "None": "None",
  "selected based on the created StorageClass:": "selected based on the created StorageClass:",
  "Discover disks": "Discover disks",
  "Security and network": "Security and network",
  "Review and create": "Review and create",
  "Internal - Attached devices": "Internal - Attached devices",
  "Can be used on any platform where there are attached devices to the nodes, using the Local Storage Operator. The infrastructure StorageClass is provided by Local Storage Operator, on top of the attached drives.": "Can be used on any platform where there are attached devices to the nodes, using the Local Storage Operator. The infrastructure StorageClass is provided by Local Storage Operator, on top of the attached drives.",
  "Node Table": "Node Table",
  "StorageCluster exists": "StorageCluster exists",
  "Back to operator page": "Back to operator page",
  "Go to cluster page": "Go to cluster page",
  "<0>A StorageCluster <1>{{clusterName}}</1> already exists.<3>You cannot create another StorageCluster.</3></0>": "<0>A StorageCluster <1>{{clusterName}}</1> already exists.<3>You cannot create another StorageCluster.</3></0>",
  "Connect to external cluster": "Connect to external cluster",
  "Download <1>{{SCRIPT_NAME}}</1> script and run on the RHCS cluster, then upload the results (JSON) in the External cluster metadata field.": "Download <1>{{SCRIPT_NAME}}</1> script and run on the RHCS cluster, then upload the results (JSON) in the External cluster metadata field.",
  "Download Script": "Download Script",
  "A bucket will be created to provide the OpenShift Data Foundation's Service.": "A bucket will be created to provide the OpenShift Data Foundation's Service.",
  "Bucket created for OpenShift Container Storage's Service": "Bucket created for OpenShift Container Storage's Service",
  "Create External StorageCluster": "Create External StorageCluster",
  "External cluster metadata": "External cluster metadata",
  "Upload JSON File": "Upload JSON File",
  "Upload Credentials file": "Upload Credentials file",
  "JSON data": "JSON data",
  "Create Button": "Create Button",
  "Create StorageCluster": "Create StorageCluster",
  "OpenShift Container Storage runs as a cloud-native service for optimal integration with applications in need of storage and handles the scenes such as provisioning and management.": "OpenShift Container Storage runs as a cloud-native service for optimal integration with applications in need of storage and handles the scenes such as provisioning and management.",
  "Select mode:": "Select mode:",
  "If not labeled, the selected nodes are labeled <1>{{label}}</1> to make them target hosts for OpenShift Container Storage's components.": "If not labeled, the selected nodes are labeled <1>{{label}}</1> to make them target hosts for OpenShift Container Storage's components.",
  "Enable arbiter": "Enable arbiter",
  "Mark nodes as dedicated": "Mark nodes as dedicated",
  "This will taint the nodes with the<1>key: node.ocs.openshift.io/storage</1>, <3>value: true</3>, and <6>effect: NoSchedule</6>": "This will taint the nodes with the<1>key: node.ocs.openshift.io/storage</1>, <3>value: true</3>, and <6>effect: NoSchedule</6>",
  "Selected nodes will be dedicated to OpenShift Container Storage use only": "Selected nodes will be dedicated to OpenShift Container Storage use only",
  "Stretch Cluster": "Stretch Cluster",
  "OpenShift Container Storage deployment in two data centers, with an arbiter node to settle quorum decisions.": "OpenShift Container Storage deployment in two data centers, with an arbiter node to settle quorum decisions.",
  "To support high availability when two data centers can be used, enable arbiter to get the valid quorum between two data centers.": "To support high availability when two data centers can be used, enable arbiter to get the valid quorum between two data centers.",
  "Arbiter minimum requirements": "Arbiter minimum requirements",
  "Select arbiter zone": "Select arbiter zone",
  "Arbiter zone selection": "Arbiter zone selection",
  "Network": "Network",
  "The default SDN networking uses a single network for all data operations such read/write and also for control plane, such as data replication. Multus allows a network separation between the data operations and the control plane operations.": "The default SDN networking uses a single network for all data operations such read/write and also for control plane, such as data replication. Multus allows a network separation between the data operations and the control plane operations.",
  "Default (SDN)": "Default (SDN)",
  "Custom (Multus)": "Custom (Multus)",
  "Public Network Interface": "Public Network Interface",
  "Select a network": "Select a network",
  "Cluster Network Interface": "Cluster Network Interface",
  "create internal mode StorageCluster wizard": "create internal mode StorageCluster wizard",
  "Can be used on any platform, except bare metal. It means that OpenShift Container Storage uses an infrastructure StorageClass, provided by the hosting platform. For example, gp2 on AWS, thin on VMWare, etc.": "Can be used on any platform, except bare metal. It means that OpenShift Container Storage uses an infrastructure StorageClass, provided by the hosting platform. For example, gp2 on AWS, thin on VMWare, etc.",
  "{{title}} steps": "{{title}} steps",
  "{{title}} content": "{{title}} content",
  "{{availableCapacity}} /  {{replica}} replicas": "{{availableCapacity}} /  {{replica}} replicas",
  "Available capacity:": "Available capacity:",
  "Filesystem name": "Filesystem name",
  "Enter filesystem name": "Enter filesystem name",
  "CephFS filesystem name into which the volume shall be created": "CephFS filesystem name into which the volume shall be created",
  "no compression": "no compression",
  "with compression": "with compression",
  "Replica {{poolSize}} {{compressionText}}": "Replica {{poolSize}} {{compressionText}}",
  "Create New Pool": "Create New Pool",
  "Storage Pool": "Storage Pool",
  "Select a Pool": "Select a Pool",
  "Storage pool into which volume data shall be stored": "Storage pool into which volume data shall be stored",
  "Error retrieving Parameters": "Error retrieving Parameters",
  "my-storage-pool": "my-storage-pool",
  "Change connection details": "Change connection details",
  "Vault Enterprise Namespace:": "Vault Enterprise Namespace:",
  "Key management service name:": "Key management service name:",
  "Provider:": "Provider:",
  "Address and Port:": "Address and Port:",
  "CA certificate:": "CA certificate:",
  "Provided": "Provided",
  "KMS service {{value}} already exist": "KMS service {{value}} already exist",
  "An encryption key will be generated for each persistent volume created using this StorageClass.": "An encryption key will be generated for each persistent volume created using this StorageClass.",
  "PV expansion operation is not supported for encrypted PVs.": "PV expansion operation is not supported for encrypted PVs.",
  "The last saved values will be updated": "The last saved values will be updated",
  "Enable Thick Provisioning": "Enable Thick Provisioning",
  "By enabling thick-provisioning, volumes will allocate the requested capacity upon volume creation. Volume creation will be slower when thick-provisioning is enabled.": "By enabling thick-provisioning, volumes will allocate the requested capacity upon volume creation. Volume creation will be slower when thick-provisioning is enabled.",
  "Used capacity": "Used capacity",
  "Storage Systems": "Storage Systems",
  "Storage status represents the health status of Openshift Data Foundation's StorageCluster.": "Storage status represents the health status of Openshift Data Foundation's StorageCluster.",
  "Health": "Health",
  "Openshift Data Foundation": "Openshift Data Foundation",
  "Standard": "Standard",
  "Data will be consumed by a Multi-cloud object gateway, deduped, compressed, and encrypted. The encrypted chunks would be saved on the selected BackingStores. Best used when the applications would always use the OpenShift Data Foundation endpoints to access the data.": "Data will be consumed by a Multi-cloud object gateway, deduped, compressed, and encrypted. The encrypted chunks would be saved on the selected BackingStores. Best used when the applications would always use the OpenShift Data Foundation endpoints to access the data.",
  "Data is stored on the NamespaceStores without performing de-duplication, compression, or encryption. BucketClasses of namespace type allow connecting to existing data and serving from them. These are best used for existing data or when other applications (and cloud-native services) need to access the data from outside OpenShift Container Storage.": "Data is stored on the NamespaceStores without performing de-duplication, compression, or encryption. BucketClasses of namespace type allow connecting to existing data and serving from them. These are best used for existing data or when other applications (and cloud-native services) need to access the data from outside OpenShift Container Storage.",
  "Single NamespaceStore": "Single NamespaceStore",
  "Multi NamespaceStores": "Multi NamespaceStores",
  "The namespace bucket will serve reads from several selected backing stores, creating a virtual namespace on top of them and will write to one of those as its chosen write target": "The namespace bucket will serve reads from several selected backing stores, creating a virtual namespace on top of them and will write to one of those as its chosen write target",
  "Cache NamespaceStore": "Cache NamespaceStore",
  "The caching bucket will serve data from a large raw data out of a local caching tiering.": "The caching bucket will serve data from a large raw data out of a local caching tiering.",
  "Create storage class": "Create storage class",
  "Create local volume set": "Create local volume set",
  "Logical used capacity per account": "Logical used capacity per account",
  "Egress Per Provider": "Egress Per Provider",
  "I/O Operations count": "I/O Operations count",
  "The infrastructure StorageClass used by OpenShift Container Storage to write its data and metadata.": "The infrastructure StorageClass used by OpenShift Container Storage to write its data and metadata.",
  "Infrastructure StorageClass created by Local Storage Operator and used by OpenShift Container Storage to write its data and metadata.": "Infrastructure StorageClass created by Local Storage Operator and used by OpenShift Container Storage to write its data and metadata.",
  "The amount of capacity that would be dynamically allocated on the infrastructure StorageClass.": "The amount of capacity that would be dynamically allocated on the infrastructure StorageClass.",
  "If you wish to use the Arbiter stretch cluster, a minimum of 4 nodes (2 different zones, 2 nodes per zone) and 1 additional zone with 1 node is required. All nodes must be pre-labeled with zones in order to be validated on cluster creation.": "If you wish to use the Arbiter stretch cluster, a minimum of 4 nodes (2 different zones, 2 nodes per zone) and 1 additional zone with 1 node is required. All nodes must be pre-labeled with zones in order to be validated on cluster creation.",
  "Selected nodes are based on the StorageClass <1>{{scName}}</1> and with a recommended requirement of 14 CPU and 34 GiB RAM per node.": "Selected nodes are based on the StorageClass <1>{{scName}}</1> and with a recommended requirement of 14 CPU and 34 GiB RAM per node.",
  "Selected nodes are based on the StorageClass <1>{{scName}}</1> and fulfill the stretch cluster requirements with a recommended requirement of 14 CPU and 34 GiB RAM per node.": "Selected nodes are based on the StorageClass <1>{{scName}}</1> and fulfill the stretch cluster requirements with a recommended requirement of 14 CPU and 34 GiB RAM per node.",
  "Block and File": "Block and File",
  "Storage": "Storage",
  "Disks": "Disks",
  "Edit Bucket Class Resources": "Edit Bucket Class Resources",
  "Loading...": "Loading...",
  "Pool {{name}} creation in progress": "Pool {{name}} creation in progress",
  "Pool {{name}} was successfully created": "Pool {{name}} was successfully created",
  "An error occurred. Pool {{name}} was not created": "An error occurred. Pool {{name}} was not created",
  "Pool {{name}} creation timed out. Please check if odf operator and rook operator are running": "Pool {{name}} creation timed out. Please check if odf operator and rook operator are running",
  "The creation of a StorageCluster is still in progress or has failed. Try again after the StorageCuster is ready to use.": "The creation of a StorageCluster is still in progress or has failed. Try again after the StorageCuster is ready to use.",
  "Pool management tasks are not supported for default pool and OpenShift Container Storage's external mode.": "Pool management tasks are not supported for default pool and OpenShift Container Storage's external mode.",
  "Pool {{name}} was created with errors.": "Pool {{name}} was created with errors.",
  "Delete": "Delete",
  "StorageClasses": "StorageClasses",
  "Compression status": "Compression status",
  "Compression savings": "Compression savings",
  "hr": "hr",
  "min": "min",
  "A minimal cluster deployment will be performed.": "A minimal cluster deployment will be performed.",
  "The selected nodes do not match OpenShift Container Storage's StorageCluster requirement of an aggregated 30 CPUs and 72 GiB of RAM. If the selection cannot be modified a minimal cluster will be deployed.": "The selected nodes do not match OpenShift Container Storage's StorageCluster requirement of an aggregated 30 CPUs and 72 GiB of RAM. If the selection cannot be modified a minimal cluster will be deployed.",
  "Back to nodes selection": "Back to nodes selection",
  "Select a StorageClass to continue": "Select a StorageClass to continue",
  "This is a required field. The StorageClass will be used to request storage from the underlying infrastructure to create the backing PersistentVolumes that will be used to provide the OpenShift Container Storage service.": "This is a required field. The StorageClass will be used to request storage from the underlying infrastructure to create the backing PersistentVolumes that will be used to provide the OpenShift Container Storage service.",
  "Create new StorageClass": "Create new StorageClass",
  "This is a required field. The StorageClass will be used to request storage from the underlying infrastructure to create the backing persistent volumes that will be used to provide the OpenShift Container Storage service.": "This is a required field. The StorageClass will be used to request storage from the underlying infrastructure to create the backing persistent volumes that will be used to provide the OpenShift Container Storage service.",
  "All required fields are not set": "All required fields are not set",
  "In order to create the StorageCluster you must set the StorageClass, select at least 3 nodes (preferably in 3 different zones) and meet the minimum or recommended requirement": "In order to create the StorageCluster you must set the StorageClass, select at least 3 nodes (preferably in 3 different zones) and meet the minimum or recommended requirement",
  "The StorageCluster requires a minimum of 3 nodes for the initial deployment. Please choose a different StorageClass or go to create a new LocalVolumeSet that matches the minimum node requirement.": "The StorageCluster requires a minimum of 3 nodes for the initial deployment. Please choose a different StorageClass or go to create a new LocalVolumeSet that matches the minimum node requirement.",
  "Create new volume set instance": "Create new volume set instance",
  "Select at least 1 encryption level or disable encryption.": "Select at least 1 encryption level or disable encryption.",
  "Fill out the details in order to connect to key management system": "Fill out the details in order to connect to key management system",
  "This is a required field.": "This is a required field.",
  "Both public and cluster network attachment definition cannot be empty": "Both public and cluster network attachment definition cannot be empty",
  "A public or cluster network attachment definition must be selected to use Multus.": "A public or cluster network attachment definition must be selected to use Multus.",
  "The number of selected zones is less than the minimum requirement of 3. If not modified a host-based failure domain deployment will be enforced.": "The number of selected zones is less than the minimum requirement of 3. If not modified a host-based failure domain deployment will be enforced.",
  "When the nodes in the selected StorageClass are spread across fewer than 3 availability zones, the StorageCluster will be deployed with the host based failure domain.": "When the nodes in the selected StorageClass are spread across fewer than 3 availability zones, the StorageCluster will be deployed with the host based failure domain.",
  "Cluster-Wide and StorageClass": "Cluster-Wide and StorageClass",
  "Cluster-Wide": "Cluster-Wide",
  "and": "and",
  "Select at least 2 Backing Store resources": "Select at least 2 Backing Store resources",
  "Select at least 1 Backing Store resource": "Select at least 1 Backing Store resource",
  "x {{replica}} replicas = {{osdSize, number}} TiB": "x {{replica}} replicas = {{osdSize, number}} TiB",
  "SmallScale": "SmallScale",
  "0.5 TiB": "0.5 TiB",
  "2 TiB": "2 TiB",
  "LargeScale": "LargeScale",
  "4 TiB": "4 TiB",
  "{{osdSize, number}} TiB": "{{osdSize, number}} TiB",
  "Help": "Help"
}
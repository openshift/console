{
  "Troubleshoot": "故障排除",
  "Add Capacity": "增加容量",
  "Object Buckets": "对象存储桶",
  "Object Bucket Claims": "对象存储桶声明",
  "Overview": "概述",
  "Use existing claim": "使用现有声明",
  "Select claim": "选择声明",
  "Create new claim": "创建新声明",
  "Create": "创建",
  "Cancel": "取消",
  "StorageSystems": "存储系统",
  "StorageSystem details": "存储系统详情",
  "Enabled": "已启用",
  "Disabled": "禁用",
  "Last synced": "最后同步",
  "BlockPool List": "块池列表",
  "Edit BlockPool": "编辑块池",
  "Delete BlockPool": "删除块池",
  "{{replica}} Replication": "{{replica}}复制",
  "Pool name": "池名称",
  "my-block-pool": "my-block-pool",
  "pool-name-help": "pool-name-help",
  "Data protection policy": "数据保护政策",
  "Select replication": "选择复制",
  "Volume type": "卷类型",
  "Select volume type": "选择卷类型",
  "Compression": "压缩",
  "Enable compression": "启用压缩",
  "Enabling compression may result in little or no space savings for encrypted or random data. Also, enabling compression may have an impact on I/O performance.": "对于加密数据和随机数据，启用压缩可能不会节省太多空间。另外，启用压缩可能会影响到 I/O 性能。",
  "OpenShift Container Storage's StorageCluster is not available. Try again after the StorageCluster is ready to use.": "OpenShift Container Storage 的存储集群不可用。在存储集群准备就绪后请再次尝试。",
  "Create BlockPool": "创建块池",
  "Close": "关闭",
  "Pool creation is not supported for OpenShift Data Foundation's external RHCS StorageSystem.": "OpenShift Data Foundation 的外部 RHCS 存储系统不支持池创建。",
  "A BlockPool is a logical entity providing elastic capacity to applications and workloads. Pools provide a means of supporting policies for access data resilience and storage efficiency.": "块池是一个逻辑实体，为应用程序和工作负载提供弹性容量。池提供了一种支持政策以访问数据恢复和存储效率的方法。",
  "BlockPool Creation Form": "块池创建表单",
  "Name": "名称",
  "Bucket Name": "存储桶名称",
  "Type": "类型",
  "Region": "区域",
  "BackingStore Table": "后端存储表",
  "Each BackingStore can be used for one tier at a time. Selecting a BackingStore in one tier will remove the resource from the second tier option and vice versa.": "每个后端存储在同一时间可用于一个层。在一个层中选择一个后端存储会从第二个层选项中删除资源，反之亦然。",
  "Bucket created for OpenShift Data Foundation's Service": "为 OpenShift Data Foundation 的服务创建的存储桶",
  "Tier 1 - BackingStores": "层 1 - 后端存储",
  "Create BackingStore ": "创建后端存储 ",
  "Tier-1-Table": "层-1-表",
  "{{bs, number}} BackingStore _0": "{{bs, number}} 后端存储",
  "selected": "已选择",
  "Tier 2 - BackingStores": "层 2 - 后端存储",
  "Tier-2-Table": "层-2-表",
  "General": "常规设置",
  "Placement Policy": "放置策略",
  "Resources": "资源",
  "Review": "复查",
  "Create BucketClass": "创建存储桶类",
  "Create new BucketClass": "创建新存储桶类",
  "BucketClass is a CRD representing a class for buckets that defines tiering policies and data placements for an OBC.": "存储桶类是一个 CRD，它代表存储桶的类，用于定义 OBC 的层策略和数据放置。",
  "Next": "下一个",
  "Back": "前一个",
  "Edit BucketClass Resource": "编辑存储桶类资源",
  "{{storeType}} represents a storage target to be used as the underlying storage for the data in Multicloud Object Gateway buckets.": "{{storeType}} 代表了一个存储对象，用作 Multicloud Object Gateway 存储桶中数据的底层存储。",
  "Cancel ": "取消 ",
  "Save": "保存",
  "Edit Bucket Class Resources": "编辑存储桶类资源",
  "What is a BackingStore?": "什么是后端存储？",
  "BackingStore represents a storage target to be used as the underlying storage for the data in Multicloud Object Gateway buckets.": "后端存储代表了一个存储对象，用作 Multicloud Object Gateway 存储桶中数据的底层存储。",
  "Multiple types of BackingStores are supported: asws-s3 s3-compatible google-cloud-storage azure-blob obc PVC.": "支持多种后端存储类型： asws-s3 s3-compatiblegoogle-cloud-storage azure-blob obc PVC。",
  "Learn More": "了解更多",
  "What is a BucketClass?": "什么是存储桶类？",
  "A set of policies which would apply to all buckets (OBCs) created with the specific bucket class. These policies include placement, namespace and caching": "一组应用到所有使用特定存储桶类创建的存储桶（OBC）策略。这些策略包括放置、命名空间和缓存",
  "BucketClass type": "存储桶类类型",
  "3-63 chars": "3-63 个字符",
  "Starts and ends with lowercase number or letter": "以小写数字或字母开始和结尾",
  "Only lowercase letters, numbers, non-consecutive periods or hyphens": "只支持小写字母、数字、非连续的句点或者连字符",
  "Avoid using the form of an IP address": "避免使用 IP 地址格式",
  "Globally unique name": "全局唯一名称",
  "BucketClass name": "存储桶类名称",
  "A unique name for the bucket class within the project.": "项目中的存储桶类的唯一名称。",
  "my-multi-cloud-mirror": "my-multi-cloud-mirror",
  "BucketClass Name": "存储桶类名称",
  "Description (Optional)": "描述（可选）",
  "Description of bucket class": "存储桶类描述",
  "What is a Namespace Policy?": "什么是命名空间策略？",
  "Namespace policy can be set to one single read and write source, multi read sources or cached policy.": "命名空间策略可为单个读写源、多读取源或缓存策略设置。",
  "Namespace Policy Type": "命名空间策略类型",
  "What is Caching?": "什么是缓存？",
  "Caching is a policy that creates local copies of the data. It saves the copies locally to improve performance for frequently accessed data. Each cached copy has a TTL and is verified against the hub. Each non-read operation (upload, overwrite, delete) is performed on the hub": "缓存是一个用于创建数据的本地副本的策略。它会在本地保存副本以提高经常访问的数据的性能。每个缓存的副本都有一个 TTL，并针对 hub 进行验证。所有非读操作（上传、覆盖、删除）都会在 hub 中执行",
  "Hub namespace store ": "hub 命名空间存储",
  "A single NamespaceStore that defines the read and write target of the namespace bucket.": "一个单一的命名空间存储，它定义了命名空间存储桶的读写目标",
  "NamespaceStore": "命名空间存储",
  "Cache data settings": "缓存数据设置",
  "The data will be temporarily copied on a backing store in order to later access it much more quickly.": "将数据临时复制到后备存储中，以便以后可以更快地访问这些数据。",
  "Backing store": "后端存储",
  "a local backing store is recommended for better performance": "为提高性能，建议使用本地后端存储",
  "Time to live": "存活时间",
  "Time to live is the time that an object is stored in a caching system before it is deleted or refreshed. Default: 1 hr, Min: 15 mins, Max: 24 hrs": "存活时间代表一个对象在删除或刷新前，存储在缓存系统中的时间。默认是： 1 hr，最小：15 mins，最大：24 hrs",
  "Read NamespaceStores": "读命名空间存储",
  "Select a list of NamespaceStores that defines the read targets of the namespace bucket.": "选择命名空间存储列表，定义命名空间存储桶的读取目标",
  "Create NamespaceStore": "创建命名空间存储",
  "{{nns, number}} namespace store _0": "{{nns, number}} 命名空间存储 ",
  " selected": " 已选择",
  "Write NamespaceStore": "写命名空间存储",
  "Select a single NamespaceStore that defines the write targets of the namespace bucket.": "选择一个单一命名空间存储，定义命名空间存储桶的写入目标",
  "Read and Write NamespaceStore ": "读和写命名空间存储",
  "Select one NamespaceStore which defines the read and write targets of the namespace bucket.": "选择一个命名空间存储，定义命名空间存储桶的读写目标",
  "What is a Placement Policy?": "放置政策是什么？",
  "Data placement capabilities are built as a multi-layer structure here are the layers bottom-up:": "数据放置功能作为多层结构构建，这里从底到顶的层：",
  "Spread Tier - list of BackingStores aggregates the storage of multiple stores.": "扩散层（Spread Tier） - 后备存储列表，聚合多个存储的存储。",
  "Mirroring Tier - list of spread-layers async-mirroring to all mirrors with locality optimization (will allocate on the closest region to the source endpoint). Mirroring requires at least two BackingStores.": "镜像层（Mirroring Tier）- 在所有带有本地优化（将在最接近的区域中分配给源端点）的分布层器 async-mirroring 列表，镜像至少需要两个后备存储。",
  "The number of replicas can be configured via the NooBaa management console.": "可以通过 NooBa 管理控制台配置的副本数量。",
  "Tier 1 - Policy Type": "层 1 - 策略类型",
  "Spread": "扩散（Spread）",
  "Spreading the data across the chosen resources. By default a replica of one copy is used and does not include failure tolerance in case of resource failure.": "在所选资源间扩散数据。默认情况使用一个副本，在资源失败时不具有容错功能。",
  "Mirror": "镜像（Mirror）",
  "Full duplication of the data in each chosen resource. By default a replica of one copy per location is used. Includes failure tolerance in case of resource failure.": "每个所选资源的数据的完全复制。默认为每个位置的每个复制使用一个副本，包括资源失败时的容错功能。",
  "Add Tier": "添加层",
  "Tier 2 - Policy type": "层 2 - 策略类型",
  "Remove Tier": "删除层",
  "Spreading the data across the chosen resources does not include failure tolerance in case of resource failure.": "在所选资源间扩散数据不包含资源失败时的容错功能。",
  "Full duplication of the data in each chosen resource includes failure tolerance in cause of resource failure.": "每个选定资源中数据的全部复制包括资源失败时的容错功能。",
  "Namespace Policy: ": "命名空间策略： ",
  "Read and write NamespaceStore : ": "读和写命名空间存储：",
  "Hub namespace store: ": "hub 命名空间存储：",
  "Cache backing store: ": "创建后端存储：",
  "Time to live: ": "存活时间： ",
  "Resources ": "资源 ",
  "Selected read namespace stores: ": "选择的读命名空间存储： ",
  "Selected write namespace store: ": "选择的写命名空间存储： ",
  "Placement policy details ": "放置策略详情",
  "Tier 1: ": "层 1：",
  "Selected BackingStores": "选择的后端存储：",
  "Tier 2: ": "层 2: ",
  "Review BucketClass": "复查存储桶类",
  "BucketClass type: ": "存储桶类类型：",
  "BucketClass name: ": "存储桶类名称：",
  "Description: ": "描述 ",
  "Provider {{provider}}": "供应商 {{provider}}",
  "Create new BackingStore ": "创建新的后端存储 ",
  "An error has occured while fetching backing stores": "获取后备存储时出错",
  "Select a backing store": "选择后端存储",
  "Storage targets that are used to store chunks of data on Multicloud Object Gateway buckets.": "用于在 Multicloud Object Gateway 存储桶中存储数据的存储目标。",
  "A BackingStore represents a storage target to be used as the underlying storage layer in Multicloud Object Gateway buckets.": "一个后端存储代表了一个存储目标，用作 Multicloud Object Gateway 存储桶中数据的底层存储层。",
  "Multiple types of BackingStores are supported: AWS S3 S3 Compatible Google Cloud Storage Azure Blob PVC.": "支持多种后备存储类型：AWS S3 S3 Compatible Google Cloud Storage Azure Blob PVC。",
  "BackingStore Name": "后端存储名称",
  "A unique name for the BackingStore  within the project": "项目中的后端存储的唯一名称",
  "Name can contain a max of 43 characters": "名称可包含最多 43 个字符",
  "Provider": "供应商",
  "Create BackingStore": "创建后端存储",
  "This is an Advanced subscription feature. It requires Advanced Edition subscription. Please contact the account team for more information.": "这是一个高级订阅功能。它需要高级订阅。请联络帐户团队以获得更多信息。",
  "Advanced Subscription": "高级订阅",
  "Advanced": "高级",
  "Deployment type": "部署类型",
  "Storage platform": "存储平台",
  "Select a storage platform you wish to connect": "选择您要连接的存储平台",
  "Select external system from list": "从列表中选择外部系统",
  "Use an existing StorageClass": "使用一个现有的存储类",
  "OpenShift Data Foundation will use an existing infrastructure StorageClass provided by your hosting platform.": "OpenShift 数据基础将使用由您的托管平台提供的现有基础架构存储类。",
  "Create a new StorageClass using local storage devices": "使用本地设备创建新存储类",
  "OpenShift Data Foundation will use an infrastructure StorageClass provided by the Local Storage Operator (LSO) on top of your attached drives. This option is available on any platform with devices attached to nodes.": "OpenShift 数据基础将在您附加的驱动器之上使用 Local Storage Operator (LSO) 提供的基础架构存储类。这个选项可在附加到节点的任意平台上使用。",
  "Connect an external storage platform": "连接一个外部存储平台",
  "OpenShift Data Foundation will create a dedicated StorageClass.": "OpenShift Data Foundation 将创建一个专用的存储类。",
  "Select capacity": "选择容量",
  "Requested capacity": "要求的容量",
  "Select nodes": "选择节点",
  "Select at least 3 nodes preferably in 3 different zones. It is recommended to start with at least 14 CPUs and 34 GiB per node.": "最少选择 3 个节点（最好在 3 个不同的区）。建议您从每个节点最少 14 个 CPU 和 34 GiB 开始。",
  "Selected capacity": "选择的容量",
  "Available raw capacity": "可用原始容量",
  "The available capacity is based on all attached disks associated with the selected StorageClass <3>{{storageClassName}}</3>": "可用容量基于与所选存储类 <3>{{storageClassName}}</3> 关联的所有附加磁盘。",
  "Selected nodes": "选中的节点",
  "Role": "角色",
  "CPU": "CPU",
  "Memory": "内存",
  "Zone": "区",
  "Selected nodes table": "选中的节点表",
  "To support high availability when two data centers can be used, enable arbiter to get a valid quorum between the two data centers.": "在两个数据中心可用时支持高可用性，启用仲裁以在两个数据中心之间获得有效的仲裁。",
  "Arbiter minimum requirements": "Arbiter 最低要求",
  "Stretch Cluster": "扩展集群",
  "Enable arbiter": "启用 arbiter",
  "Arbiter zone": "仲裁区：",
  "An arbiter node will be automatically selected from this zone": "将从此区中自动选择一个仲裁节点",
  "Select an arbiter zone": "选择 arbiter 区",
  "Arbiter zone selection": "Arbiter 区选择",
  "Connection details": "连接详情",
  "Disks on all nodes": "所有节点上的磁盘",
  "{{nodes, number}} node_0": "{{nodes, number}} 节点",
  "Please enter a positive Integer": "请输入一个正的整数",
  "LocalVolumeSet name": "本地卷集名称",
  "A LocalVolumeSet will be created to allow you to filter a set of disks, group them and create a dedicated StorageClass to consume storage from them.": "本地卷集将被创建以允许您过滤一组磁盘并对其进行分组，并创建一个专用存储类来使用它们的存储。",
  "StorageClass name": "存储类名称",
  "Filter disks by": "过滤磁盘",
  "Uses the available disks that match the selected filters on all nodes.": "使用在所有节点上与所选过滤器相匹配的可用磁盘。",
  "Disks on selected nodes": "在所选节点上的磁盘",
  "Uses the available disks that match the selected filters only on selected nodes.": "使用在所选节点上与所选过滤器相匹配的可用磁盘。",
  "Disk type": "磁盘类型",
  "Volume mode": "卷模式",
  "Device type": "设备类型",
  "Select disk types": "选择磁盘类型",
  "Disk size": "磁盘大小",
  "Minimum": "最小",
  "Please enter a value less than or equal to max disk size": "请输入小于或等于最大磁盘大小的值",
  "Maximum": "最大",
  "Please enter a value greater than or equal to min disk size": "请输入大于或等于最小磁盘大小的值",
  "Units": "单元",
  "Maximum disks limit": "最大磁盘限制",
  "Disks limit will set the maximum number of PVs to create on a node. If the field is empty we will create PVs for all available disks on the matching nodes.": "磁盘限制将设置节点上创建的最大 PV 数量。如果字段为空，将为匹配节点上的所有可用磁盘创建 PV。",
  "All": "所有",
  "Local Storage Operator not installed": "没有安装 Local Storage Operator。",
  "Before we can create a StorageSystem, the Local Storage Operator needs to be installed. When installation is finished come back to OpenShift Data Foundation to create a StorageSystem.<1><0>Install</0></1>": "在创建存储系统前，需要安装 Local Storage Operator。当完成安装后，返回到 OpenShift Data Foundation 来创建一个存储系统。<1><0>安装</0></1>",
  "Checking Local Storage Operator installation": "检查 Local Storage Operator 的安装",
  "Discovering disks on all hosts. This may take a few minutes.": "发现所有主机上的磁盘。这可能需要几分钟时间。",
  "Minimum Node Requirement": "最低节点要求",
  "A minimum of 3 nodes are required for the initial deployment. Only {{nodes}} node match to the selected filters. Please adjust the filters to include more nodes.": "初始的部署最少需要 3 个节点进行部署。只有 {{nodes}} 节点与所选过滤器匹配。请调整过滤器使其包含更多节点。",
  "After the LocalVolumeSet is created you won't be able to go back to this step.": "在创建了本地卷集后，您将无法返回到这一步。",
  "Note:": "备注：",
  "Create LocalVolumeSet": "创建本地卷集",
  "Yes": "是",
  "Are you sure you want to continue?": "您确定要继续吗？",
  "Node": "节点",
  "Model": "型号",
  "Capacity": "容量",
  "Selected Disks": "所选磁盘",
  "Disk List": "任务列表",
  "{{nodes, number}} Node_0": "{{nodes, number}} 个节点",
  "{{disks, number}} Disk_0": "{{disks, number}} 个磁盘",
  "Selected versus Available Capacity": "选择的容量与可用容量的比较",
  "Out of {{capacity}}": "超出 {{capacity}}",
  "{{displayName}} connection details": "{{displayName}} 连接详情",
  "Not connected": "未连接",
  "Backing storage": "后端存储",
  "StorageClass: {{name}}": "存储类：{{name}}",
  "Deployment type: {{deployment}}": "部署类型：{{deployment}}",
  "External storage platform: {{storagePlatform}}": "外部存储平台：{{storagePlatform}}",
  "Capacity and nodes": "容量和节点",
  "Cluster capacity: {{capacity}}": "集群容量：{{capacity}}",
  "Selected nodes: {{nodeCount, number}} node_0": "选中的节点：{{nodeCount, number}} 个节点",
  "CPU and memory: {{cpu, number}} CPU and {{memory}} memory": "CPU 和内存：{{cpu, number}} 个 CPU 和 {{memory}} 内存",
  "Zone: {{zoneCount, number}} zone_0": "时区：{{zoneCount, number}} 时区",
  "Arbiter zone: {{zone}}": "仲裁区：{{zone}}",
  "Security": "安全性",
  "Encryption: Enabled": "加密：启用",
  "External key management service: {{kmsStatus}}": "外部密钥管理服务： {{kmsStatus}}",
  "Security and network": "安全和网络",
  "Encryption: {{encryptionStatus}}": "加密：{{encryptionStatus}}",
  "Network: {{networkType}}": "网络：{{networkType}}",
  "Encryption level": "加密级别",
  "The StorageCluster encryption level can be set to include all components under the cluster (including StorageClass and PVs) or to include only StorageClass encryption. PV encryption can use an auth token that will be used with the KMS configuration to allow multi-tenancy.": "存储集群加密级别可设置为包含集群中的所有组件（包括存储类和 PV），或仅包含存储类加密。PV 加密可以使用与 KMS 配置搭配使用的身份验证令牌来允许多租户。",
  "Cluster-wide encryption": "集群范围的加密",
  "Encryption for the entire cluster (block and file)": "整个集群的加密（块和文件）",
  "StorageClass encryption": "存储类加密",
  "An encryption key will be generated for each persistent volume (block) created using an encryption enabled StorageClass.": "将创建一个加密密钥，用于每个使用一个启用了加密的存储类创建的持久性卷（块）。",
  "Connection settings": "连接设置",
  "Connect to an external key management service": "连接到外部密钥管理服务",
  "Data encryption for block and file storage. MultiCloud Object Gateway is always encrypted.": "块和文件存储的数据加密。MultiCloud Object Gateway 总会被加密。",
  "MultiCloud Object Gateway is always encrypted.": "多云对象网关始终被加密。",
  "Enable data encryption for block and file storage": "为块和文件存储启用数据加密",
  "Enable encryption": "启用加密",
  "Encryption": "Encryption",
  "An error has occurred: {{error}}": "出错：{{error}}",
  "IP address": "IP 地址",
  "Rest API IP address of IBM FlashSystem.": "IBM FlashSystem 的 REST API IP 地址。",
  "The endpoint is not a valid IP address": "端点不是一个有效的 IP 地址",
  "Username": "用户名",
  "Password": "密码",
  "Hide password": "隐藏密码",
  "Reveal password": "显示密码",
  "The uploaded file is not a valid JSON file": "上传的文件不是有效的 JSON 文件",
  "External storage system metadata": "外部存储系统元数据",
  "Download <1>{{SCRIPT_NAME}}</1> script and run on the RHCS cluster, then upload the results (JSON) in the External storage system metadata field.": "下载 <1>{{SCRIPT_NAME}}</1> 脚本并在 RHCS 集群上运行，然后在外部存储系统元数据字段上传结果（JSON）。",
  "Download script": "下载脚本",
  "Browse": "浏览",
  "Clear": "清理",
  "Upload helper script": "上传帮助程序脚本",
  "An error has occurred": "出错",
  "Create StorageSystem": "创建存储系统",
  "Create a StorageSystem to represent your OpenShift Data Foundation system and all its required storage and computing resources.": "创建存储系统以代表您的 OpenShift Data Foundation 系统及其所有必要的存储和计算资源。",
  "{{nodeCount, number}} node_0": "{{nodeCount, number}} 个节点",
  "selected ({{cpu}} CPU and {{memory}} on ": "选择的（{{cpu}} CPU 和 {{memory}} 于 ",
  "{{zoneCount, number}} zone_0": "{{zoneCount, number}} 区",
  "Search by node name...": "按节点名称搜索......",
  "Search by node label...": "按节点标签搜索......",
  "Not found": "没有找到",
  "Compression eligibility": "压缩资格",
  "Compression eligibility indicates the percentage of incoming data that is compressible": "压缩资格表示传入数据中可压缩数据所占的百分比",
  "Compression savings": "压缩节省",
  "Compression savings indicates the total savings gained from compression for this pool, including replicas": "压缩节省表示此池压缩实现的总节省，包括副本",
  "Compression ratio": "压缩比例",
  "Compression ratio indicates the achieved compression on eligible data for this pool": "压缩率表示对这个池的有压缩资格的数据的压缩",
  "Compression status": "压缩状态",
  "Storage efficiency": "存储效率",
  "Details": "详情",
  "Replicas": "副本",
  "Inventory": "库存",
  "Not available": "不可用",
  "Image states info": "镜像状态信息",
  "What does each state mean?": "每种状态的含义是什么？",
  " <1>Starting replay:</1> Initiating image (PV) replication process. ": " <1>Starting replay:</1> 启动镜像（PV）复制过程. ",
  " <1>Replaying:</1> Image (PV) replication is ongoing or idle between clusters. ": " <1>Replaying:</1> 镜像（PV）复制在集群间进行或闲置。 ",
  " <1>Stopping replay:</1> Image (PV) replication process is shutting down. ": " <1>Stopping replay:</1> 镜像（PV）复制过程正在停止。 ",
  " <1>Stopped:</1> Image (PV) replication process has shut down. ": " <1>Stopped:</1> 镜像（PV）已停止。",
  " <1>Error:</1> Image (PV) replication process stopped due to an error. ": " <1>Error:</1> 镜像（PV）复制过程因为错误而停止。 ",
  " <1>Unknown:</1> Unable to determine image (PV) state due to an error. Check your network connection and remote cluster mirroring daemon. ": " <1>Unknown:</1> 因为错误无法决定镜像（PV）的状态。检查您的网络连接和远程集群镜像守护进程。 ",
  "image states info": "镜像状态信息",
  "Image States": "镜像状态",
  "Mirroring": "镜像",
  "Mirroring status": "镜像状态",
  "Overall image health": "镜像的整体健康状况",
  "Show image states": "显示镜像状态",
  "Last checked": "最后检查",
  "Raw Capacity shows the total physical capacity from all storage media within the storage subsystem": "原始容量显示存储子系统内所有存储介质的物理容量总量",
  "Start replay": "开始重播",
  "Stop reply": "停止回复",
  "Replaying": "正在重播",
  "Stopped": "已停止",
  "Error": "错误",
  "Syncing": "同步",
  "Unknown": "未知",
  "Status": "状态",
  "Performance": "性能",
  "IOPS": "IOPS",
  "Throughput": "吞吐量",
  "Not enough usage data": "没有足够的使用数据",
  "used": "使用的",
  "available": "可用",
  "Other": "其他",
  "All other capacity usage that are not a part of the top 5 consumers.": "不是顶级 5 个用户的一部分的所有其他容量用量。",
  "Available": "可用",
  "Breakdown Chart": "分解图",
  "Warning": "警告",
  "Raw capacity": "原始容量",
  "Used": "使用的",
  "Available versus Used Capacity": "可用和已使用的容量对比",
  "Used of {{capacity}}": "使用 {{capacity}}",
  "Not Available": "不可用",
  "Rebuilding data resiliency": "重建数据弹性",
  "{{formattedProgress, number}}%": "{{formattedProgress, number}}%",
  "Activity": "活跃",
  "Estimating {{formattedEta}} to completion": "估算 {{formattedEta}} 到完成",
  "Object_0": "对象",
  "Buckets": "存储桶",
  "Buckets card represents the number of S3 buckets managed on Multicloud Object Gateway and the number of ObjectBucketClaims and the ObjectBuckets managed on both Multicloud Object Gateway and RGW (if deployed).": "存储桶卡包括了在多云对象网关中管理的 S3 存储桶的数量、对象存储桶声明的数量，以及在多云对象网关和 RGW（如果部署）上管理的对象存储桶的数量。",
  "NooBaa Bucket": "NooBaa 存储桶",
  "Break by": "由分解",
  "Total": "总计",
  "Projects": "项目",
  "BucketClasses": "存储桶类",
  "Service type": "服务类型",
  "Cluster-wide": "集群范围的",
  "Any NON Object bucket claims that were created via an S3 client or via the NooBaa UI system.": "通过 S3 客户端或通过 NooBaa UI 系统创建的任何 NON 对象存储桶声明。",
  "Capacity breakdown": "容量分解",
  "This card shows used capacity for different resources. The available capacity is based on cloud services therefore it cannot be shown.": "这个卡显示不同资源的使用容量。可用容量基于云服务，因此无法显示。",
  "Type: {{serviceType}}": "类型：{{serviceType}}",
  "Service Type Dropdown": "服务类型下拉菜单",
  "Service Type Dropdown Toggle": "服务类型下拉菜单切换",
  "By: {{serviceType}}": "按：{{serviceType}}",
  "Break By Dropdown": "按下拉菜单分解",
  "Providers": "供应商",
  "Accounts": "帐户",
  "Metric": "指标",
  "I/O Operations": "I/O 操作",
  "Logial Used Capacity": "逻辑使用容量",
  "Physical vs. Logical used capacity": "物理使用容量与逻辑使用容量比较",
  "Egress": "Egress",
  "Latency": "延迟",
  "Bandwidth": "带宽",
  "Service Type": "服务类型",
  "Type: {{selectedService}}": "类型：{{selectedService}}",
  "{{selectedMetric}} by {{selectedBreakdown}}": "通过 {{selectedBreakdown}} 的 {{selectedMetric}}",
  "thousands": "数千",
  "millions": "数百万",
  "billions": "数十亿",
  "Total Reads {{totalRead}}": "总读操作 {{totalRead}}",
  "Total Writes {{totalWrite}}": "总写操作 {{totalWrite}}",
  "Total Logical Used Capacity {{logicalCapacity}}": "逻辑使用的总容量 {{logicalCapacity}}",
  "Total Physical Used Capacity {{physicalcapacity}}": "物理使用的总容量 {{physicalcapacity}}",
  "Shows an overview of the data consumption per provider or account collected from the day of the entity creation.": "显示在创建实体时收集的每个供应商或帐户的数据消耗概述。",
  "(in {{suffixLabel}})": "(在 {{suffixLabel}})",
  "Data Consumption Graph": "数据消耗图",
  "GET {{GETLatestValue}}": "GET {{GETLatestValue}}",
  "PUT {{PUTLatestValue}}": "PUT {{PUTLatestValue}}",
  "OpenShift Data Foundation": "OpenShift Data Foundation",
  "OpenShift Container Storage": "OpenShift Container Storage",
  "Service Name": "服务名称",
  "System Name": "系统名称",
  "Multicloud Object Gateway": "多云对象网关（MCG）",
  "RADOS Object Gateway": "RADOS 对象网关",
  "Version": "版本",
  "Resource Providers": "资源供应商",
  "A list of all Multicloud Object Gateway resources that are currently in use. Those resources are used to store data according to the buckets' policies and can be a cloud-based resource or a bare metal resource.": "当前正在使用的所有 Multicloud Object Gateway 资源列表。这些资源用于根据存储桶策略存储数据，并可以是基于云的资源或裸机资源。",
  "Object Service": "对象服务",
  "Data Resiliency": "数据弹性",
  "Object Service Status": "对象服务状态",
  "The object service includes 2 services.": "对象服务包括 2 个服务。",
  "The data resiliency includes 2 services": "数据弹性包括 2 个服务",
  "Services": "服务",
  "Object Gateway (RGW)": "对象网关（RGW）",
  "All resources are unhealthy": "所有资源都不健康",
  "Object Bucket has an issue": "对象存储桶有问题",
  "Many buckets have issues": "很多存储桶有问题",
  "Some buckets have issues": "有些存储桶有问题",
  "{{capacityRatio, number}}:1": "{{capacityRatio, number}}:1",
  "OpenShift Data Foundation can be configured to use compression. The efficiency rate reflects the actual compression ratio when using such a configuration.": "OpenShift Data Foundation 可以配置为使用压缩。在使用这种配置时，效率比率反映了实际的压缩比例。",
  "Savings": "节省",
  "Savings shows the uncompressed and non-deduped data that would have been stored without those techniques.": "节省显示在没有使用这些技术的情况下，未压缩的数据",
  "Storage Efficiency": "存储效率",
  "OpenShift Container Storage Overview": "OpenShift Container Storage 概述",
  "Block and File": "块和文件",
  "BlockPools": "块池",
  "Storage Classes": "存储类",
  "Pods": "Pod",
  "{{metricType}}": "{{metricType}}",
  "Break by dropdown": "按下拉菜单分解",
  "Cluster Name": "集群名称",
  "Mode": "模式",
  "Storage Cluster": "存储集群",
  "Utilization": "使用率",
  "Used Capacity": "使用的容量",
  "Expanding StorageCluster": "扩展存储集群",
  "Upgrading OpenShift Data Foundation's Operator": "升级 OpenShift Data Foundation 的 Operator",
  "Used Capacity Breakdown": "使用容量分解",
  "This card shows the used capacity for different Kubernetes resources. The figures shown represent the Usable storage, meaning that data replication is not taken into consideration.": "此卡显示了不同 Kubernetes 资源的使用容量。显示中的内容代表了可用的存储，这意味着数据复制不会被考虑。",
  "Service name": "服务名称",
  "Cluster name": "集群名称",
  "Internal": "内部",
  "Raw capacity is the absolute total disk space available to the array subsystem.": "原始容量是阵列子系统可用的绝对磁盘空间。",
  "Active health checks": "主动健康状态检查",
  "Progressing": "进行中",
  "The Compression Ratio represents the compressible data effectiveness metric inclusive of all compression-enabled pools.": "Compression Ratio 代表所有启用了压缩的池的压缩数据机制指标。",
  "The Savings metric represents the actual disk capacity saved inclusive of all compression-enabled pools and associated replicas.": "Savings 指标代表保存的实际磁盘容量，其中包括所有启用了压缩的池和相关副本。",
  "Performance metrics over time showing IOPS, Latency and more. Each metric is a link to a detailed view of this metric.": "性能指标数据显示 IOPS、Latency 等信息。每个指标数据都是一个指向这个指标的详细视图的链接。",
  "Recovery": "恢复",
  "Disk State": "磁盘状态",
  "OpenShift Data Foundation status": "OpenShift Data Foundation 状态",
  "Filesystem": "文件系统",
  "Disks List": "磁盘列表",
  "Start Disk Replacement": "启动磁盘替换",
  "<0>{{diskName}}</0> can be replaced with a disk of same type.": "<0>{{diskName}}</0> 可使用同类磁盘替换。",
  "Troubleshoot disk <1>{{diskName}}</1>": "故障排除磁盘 <1>{{diskName}}</1>",
  "here": "此处",
  "Online": "在线",
  "Offline": "离线",
  "NotResponding": "NotResponding",
  "PreparingToReplace": "PreparingToReplace",
  "ReplacementFailed": "ReplacementFailed",
  "ReplacementReady": "ReplacementReady",
  "This is a required field": "这是必填字段",
  "Please enter a URL": "请输入一个 URL",
  "Please enter a valid port": "请输入一个有效端口",
  "Connect to a Key Management Service": "连接到一个密钥管理服务",
  "Key management service provider": "密钥管理服务供应商",
  "kms-provider-name": "kms-provider-name",
  "A unique name for the key management service within the project.": "项目中密钥管理服务的唯一名称。",
  "Address": "地址",
  "Port": "端口",
  "Token": "令牌",
  "Hide token": "隐藏令牌",
  "Reveal token": "显示令牌",
  "Advanced Settings": "高级设置",
  "Raw Capacity": "原始容量",
  "x {{ replica, number }} replicas =": "x {{ replica, number }} 副本 =",
  "No StorageClass selected": "没有选择存储类",
  "The Arbiter stretch cluster requires a minimum of 4 nodes (2 different zones, 2 nodes per zone). Please choose a different StorageClass or create a new LocalVolumeSet that matches the minimum node requirement.": "Arbiter 扩展集群至少需要 4 个节点（2 个不同区，每个区 2 个节点）。请选择不同的 StorageClass 或创建一个符合最低节点要求的新 LocalVolumeSet。",
  "The StorageCluster requires a minimum of 3 nodes. Please choose a different StorageClass or create a new LocalVolumeSet that matches the minimum node requirement.": "存储集群要求最少 3 个节点。请选择不同的存储集群或创建一个符合最低节点要求的新的本地卷集。",
  "Adding capacity for <1>{{name}}</1>, may increase your expenses.": "为 <1>{{name}}</1> 增加容量，可能会增加您的费用。",
  "StorageClass": "存储类",
  "Currently Used:": "目前使用的：",
  "Add": "添加",
  "Vault enterprise namespaces are isolated environments that functionally exist as Vaults within a Vault. They have separate login paths and support creating and managing data isolated to their namespace.": "Vault 企业命名空间是作为 Vaults 存在于 Vault 里的隔离的环境。它们有单独的登录路径，并支持创建和管理与其命名空间隔离的数据。",
  "Maximum file size exceeded. File limit is 4MB.": "超过了最大文件大小。文件限制为 4MB。",
  "A PEM-encoded CA certificate file used to verify the Vault server's SSL certificate.": "用于验证 Vault 服务器 SSL 证书的 PEM 编码的 CA 证书文件。",
  "A PEM-encoded client certificate. This certificate is used for TLS communication with the Vault server.": "PEM 编码的客户端证书。此证书用于与 Vault 服务器的 TLS 通信。",
  "An unencrypted, PEM-encoded private key which corresponds to the matching client certificate provided with VAULT_CLIENT_CERT.": "与 VAULT_CLIENT_CERT 提供的匹配客户端证书对应的未加密 PEM 编码私钥。",
  "The name to use as the SNI host when OpenShift Data Foundation connecting via TLS to the Vault server": "当 OpenShift Data Foundation 通过 TLS 连接到 Vault 服务器时用作 SNI 主机的名称",
  "Key Management Service Advanced Settings": "密钥管理服务高级设置",
  "Backend Path": "后端路径",
  "path/": "path/",
  "TLS Server Name": "TLS 服务器名称",
  "Vault Enterprise Namespace": "Vault 企业命名空间",
  "The name must be accurate and must match the service namespace": "名称必须准确且必须与服务命名空间匹配",
  "CA Certificate": "CA 证书",
  "Upload a .PEM file here...": "在这里上传一个 .PEM 文件...",
  "Client Certificate": "客户端证书",
  "Client Private Key": "客户端私钥",
  "Attach OBC to a Deployment": "把 OBC 附加到部署中",
  "Deployment Name": "部署名称",
  "Attach": "附加",
  "<0><0>{{poolName}}</0> cannot be deleted. When a pool is bounded to PVC it cannot be deleted. Please detach all the resources from StorageClass(es):</0>": "<0><0>{{poolName}}</0> 无法删除。当池绑定到 PVC 时无法删除它。请从存储类中分离所有资源： </0>",
  "<0>Deleting <1>{{poolName}}</1> will remove all the saved data of this pool. Are you sure want to delete?</0>": "<0>删除 <1>{{poolName}}</1> 将删除此池的所有保存数据。您确定要删除吗？</0>",
  "BlockPool Delete Modal": "块池删除模态",
  "Try Again": "再试",
  "Finish": "完成",
  "Go To Pvc List": "进入 Pvc 列表",
  "BlockPool Update Form": "块池更新表单",
  "replacement disallowed: disk {{diskName}} is {{replacingDiskStatus}}": "替换禁用的：磁盘 {{diskName}} 是 {{replacingDiskStatus}}",
  "replacement disallowed: disk {{diskName}} is {{replacementStatus}}": "替换禁用的：磁盘 {{diskName}} 是 {{replacementStatus}}",
  "Disk Replacement": "替换磁盘",
  "This action will start preparing the disk for replacement.": "这个操作将开始为磁盘准备替换。",
  "Data rebalancing is in progress": "数据重新平衡正在进行中",
  "See data resiliency status": "请参阅数据弹性状态",
  "Are you sure you want to replace <1>{{diskName}}</1>?": "您确定要替换 <1>{{diskName}}</1>？",
  "Replace": "替换",
  "Create NamespaceStore ": "创建命名空间存储 ",
  "Represents an underlying storage to be used as read or write target for the data in the namespace buckets.": "代表一个底层存储，作为命名空间存储桶中的数据读取或写入目标。",
  "Provider {{provider}} | Region: {{region}}": "供应商 {{provider}} | 区域： {{region}}",
  "Create new NamespaceStore ": "创建新的命名空间存储",
  "An error has occurred while fetching namespace stores": "获取命名空间存储时出现错误",
  "Select a namespace store": "选择一个命名空间存储",
  "Namespace store name": "命名空间存储名称",
  "A unique name for the namespace store within the project": "项目中的命名空间存储的唯一名称",
  "Namespace Store Table": "命名空间存储表",
  "Where can I find Google Cloud credentials?": "在哪里可以找到 Google Cloud 云凭证？",
  "Service account keys are needed for Google Cloud Storage authentication. The keys can be found in the service accounts page in the GCP console.": "Google Cloud Storage 身份验证需要服务帐户密钥。这些密钥可以在 GCP 控制台的服务帐户页面中找到。",
  "Learn more": "了解更多",
  "Upload a .json file with the service account keys provided by Google Cloud Storage.": "上传一个包括由 Google Cloud Storage 提供的服务帐户密钥的 .json 文件。",
  "Secret Key": "Secret 键",
  "Upload JSON": "上传 JSON",
  "Uploaded File Name": "上传的文件名",
  "Upload File": "上传文件",
  "Switch to Secret": "切换到 Secret",
  "Select Secret": "选择 Secret",
  "Switch to upload JSON": "切换到上传 JSON",
  "Cluster Metadata": "集群元数据",
  "Target Bucket": "目标存储桶",
  "Number of Volumes": "卷数量",
  "Volume Size": "卷大小",
  "Target blob container": "目标 blob 容器",
  "Target bucket": "目标存储桶",
  "Account name": "服务帐户名称",
  "Access key": "访问密钥",
  "Account key": "帐户密钥",
  "Secret key": "Secret 键",
  "Region Dropdown": "区域下拉菜单",
  "Endpoint": "端点",
  "Endpoint Address": "端点地址",
  "Secret": "Secret",
  "Switch to Credentials": "切换到凭证",
  "Access Key Field": "访问密钥字段",
  "Secret Key Field": "Secret 键字段",
  "ObjectBucketClaim Name": "对象存储桶声明名称",
  "my-object-bucket": "my-object-bucket",
  "If not provided a generic name will be generated.": "如果没有提供，则会生成一个通用名称。",
  "Defines the object-store service and the bucket provisioner.": "定义对象存储服务和存储桶置备程序。",
  "BucketClass": "存储桶类",
  "Select BucketClass": "选择存储桶类",
  "Create ObjectBucketClaim": "创建对象存储桶声明",
  "Edit YAML": "编辑 YAML",
  "Attach to Deployment": "附加到部署",
  "Object Bucket Claim Details": "对象存储桶声明详情",
  "Object Bucket": "对象存储桶",
  "Namespace": "命名空间",
  "OBCTableHeader": "OBCTableHeader",
  "Object Bucket Claim Data": "对象存储桶声明数据",
  "Hide Values": "隐藏值",
  "Reveal Values": "显示值",
  "Data": "数据",
  "Create Object Bucket": "创建对象存储桶",
  "Object Bucket Name": "对象存储桶名称",
  "ob-name-help": "ob-name-help",
  "Object Bucket Details": "对象存储桶详情",
  "Object Bucket Claim": "对象存储桶声明",
  "OBTableHeader": "OBTableHeader",
  "Uses the available disks that match the selected filters on all nodes selected in the previous step.": "使用与在上一步中选择的所有节点上选择的过滤器匹配的可用磁盘。",
  "A LocalVolumeSet allows you to filter a set of disks, group them and create a dedicated StorageClass to consume storage from them.": "本地卷集允许您过滤一组磁盘对其进行分组，并创建一个专用存储类来使用它们的存储。",
  "OpenShift Container Storage's StorageCluster requires a minimum of 3 nodes for the initial deployment. Only {{nodes}} node match to the selected filters. Please adjust the filters to include more nodes.": "OpenShift Container Storage 的存储集群最少需要 3 个节点进行初始部署。只有 {{nodes}} 个节点与所选过滤器匹配。请调整过滤器使其包含更多节点。",
  "After the LocalVolumeSet and StorageClass are created you won't be able to go back to this step.": "在创建了本地卷集和存储类后，您将无法返回到这一步。",
  "Create StorageClass": "创建存储类",
  "Selected Capacity": "选择的容量",
  "Selected Nodes": "选中的节点",
  "Review StorageCluster": "查看存储集群",
  "Storage and nodes": "存储和节点",
  "Arbiter zone:": "Arbiter 区：",
  "None": "无",
  "selected based on the created StorageClass:": "根据创建的存储类选择：",
  "Total CPU and memory of {{cpu, number}} CPU and {{memory}}": "CPU 和内存总量 {{cpu, number}} CPU 和 {{memory}}",
  "Configure": "配置",
  "Enable Encryption": "启用加密",
  "Connect to external key management service: {{name}}": "连接到外部密钥管理服务： {{name}}",
  "Encryption Level: {{level}}": "加密级别：{{level}}",
  "Using {{networkLabel}}": "使用 {{networkLabel}}",
  "Discover disks": "发现磁盘",
  "Review and create": "查看并创建",
  "Internal - Attached devices": "内部 - 附加的设备",
  "Can be used on any platform where there are attached devices to the nodes, using the Local Storage Operator. The infrastructure StorageClass is provided by Local Storage Operator, on top of the attached drives.": "可以通过 Local Storage Operator 在将节设备附加到节点的任何平台中使用。基础架构存储类由 Local Storage Operator 在附加的驱动器上提供。",
  "Before we can create a StorageCluster, the Local Storage operator needs to be installed. When installation is finished come back to OpenShift Container Storage to create a StorageCluster.<1><0>Install</0></1>": "在创建存储集群前，需要安装 Local Storage operator。当完成安装后，请返回 OpenShift Container Storage 来创建一个存储集群。<1><0>安装</0></1>",
  "Node Table": "节点表",
  "StorageCluster exists": "存在存储集群",
  "Back to operator page": "返回 operator 页",
  "Go to cluster page": "进入集群页",
  "<0>A StorageCluster <1>{{clusterName}}</1> already exists.<3>You cannot create another StorageCluster.</3></0>": "<0>存储集群 <1>{{clusterName}}</1> 已存在。<3>无法创建另一个存储集群。</3></0>",
  "Connect to external cluster": "连接到外部集群",
  "Download <1>{{SCRIPT_NAME}}</1> script and run on the RHCS cluster, then upload the results (JSON) in the External cluster metadata field.": "下载 <1>{{SCRIPT_NAME}}</1> 脚本并在 RHCS 集群上运行，然后在外部集群元数据字段上传结果（JSON）。",
  "Download Script": "下载脚本",
  "A bucket will be created to provide the OpenShift Data Foundation's Service.": "将创建一个存储桶来提供 OpenShift Data Foundation 的服务。",
  "Bucket created for OpenShift Container Storage's Service": "为 OpenShift Container Storage 的服务创建的存储桶",
  "Create External StorageCluster": "创建外部存储集群",
  "External cluster metadata": "外部集群元数据",
  "Upload JSON File": "上传 JSON 文件",
  "Upload Credentials file": "上传凭证文件",
  "JSON data": "JSON 数据",
  "Create Button": "创建按钮",
  "Create StorageCluster": "创建存储集群",
  "OpenShift Container Storage runs as a cloud-native service for optimal integration with applications in need of storage and handles the scenes such as provisioning and management.": "OpenShift Container Storage 作为一个云原生服务运行，可优化与需要存储的应用程序集成，并处理场景，如置备和管理。",
  "Select mode:": "选择模式：",
  "If not labeled, the selected nodes are labeled <1>{{label}}</1> to make them target hosts for OpenShift Container Storage's components.": "如果没有标记，所选节点会标记为 <1>{{label}}</1>，使其为 OpenShift Container Storage 组件的目标主机。",
  "Mark nodes as dedicated": "将节点标记为专用",
  "This will taint the nodes with the<1>key: node.ocs.openshift.io/storage</1>, <3>value: true</3>, and <6>effect: NoSchedule</6>": "这将使用<1>key: node.ocs.openshift.io/storage</1>、<3>value: true</3> 和 <6>effect: NoSchedule</6> 来对节点进行污点设置",
  "Selected nodes will be dedicated to OpenShift Container Storage use only": "所选节点仅供 OpenShift Container Storage 使用",
  "OpenShift Container Storage deployment in two data centers, with an arbiter node to settle quorum decisions.": "OpenShift Container Storage 部署为两个数据中心，其具有一个空闲节点来处理仲裁决策。",
  "To support high availability when two data centers can be used, enable arbiter to get the valid quorum between two data centers.": "在两个数据中心可用时支持高可用性，启用 arbiter 以在两个数据中心之间获得有效的仲裁。",
  "Select arbiter zone": "选择 arbiter 区",
  "Network": "网络",
  "The default SDN networking uses a single network for all data operations such read/write and also for control plane, such as data replication. Multus allows a network separation between the data operations and the control plane operations.": "默认 SDN 网络将单个网络用于所有数据操作，如读/写，也用于控制平面。Multus 允许在数据操作和控制平面操作间进行网络分离。",
  "Default (SDN)": "默认 (SDN)",
  "Custom (Multus)": "自定义 (Multus)",
  "Public Network Interface": "公共网络接口",
  "Select a network": "选择一个网络",
  "Cluster Network Interface": "集群网络接口",
  "Requested Cluster Capacity:": "请求的集群容量：",
  "StorageClass:": "存储类：",
  "Select Capacity": "选择容量",
  "Requested Capacity": "要求的容量",
  "Select Nodes": "选择节点",
  "create internal mode StorageCluster wizard": "创建内部模式存储集群向导",
  "Can be used on any platform, except bare metal. It means that OpenShift Container Storage uses an infrastructure StorageClass, provided by the hosting platform. For example, gp2 on AWS, thin on VMWare, etc.": "可在除裸机外的任何平台中使用。它表示 OpenShift Container Storage 使用由主机平台提供的基础架构存储类。例如，AWS 上的 gp2、VMWare 上的精简等。",
  "{{title}} steps": "{{title}} 步骤",
  "{{title}} content": "{{title}} 内容",
  "{{availableCapacity}} /  {{replica}} replicas": "{{availableCapacity}} /  {{replica}} 副本",
  "Available capacity:": "可用容量：",
  "Filesystem name": "文件系统名称",
  "Enter filesystem name": "输入文件系统名称",
  "CephFS filesystem name into which the volume shall be created": "卷应该被创建到的 cephFS 文件系统名称",
  "no compression": "没有压缩",
  "with compression": "有压缩",
  "Replica {{poolSize}} {{compressionText}}": "副本 {{poolSize}} {{compressionText}}",
  "Create New Pool": "创建新池",
  "Storage Pool": "存储池",
  "Select a Pool": "选择一个池",
  "Storage pool into which volume data shall be stored": "保存卷数据的存储池",
  "Error retrieving Parameters": "获取参数时出错",
  "my-storage-pool": "my-storage-pool",
  "An encryption key will be generated for each PersistentVolume created using this StorageClass.": "将创建一个加密密钥，用于每个使用这个存储类创建的持久性卷。",
  "Select an existing connection": "选择一个现有的连接",
  "KMS service {{value}} already exist": "KMS 服务 {{value}} 已存在",
  "Choose existing KMS connection": "选择现有的 KMS 连接",
  "Create new KMS connection": "创建新的 KMS 连接",
  "PV expansion operation is not supported for encrypted PVs.": "加密的 PV 不支持 PV 扩展操作。",
  "Enable Thick Provisioning": "启用 Thick Provisioning",
  "By enabling thick-provisioning, volumes will allocate the requested capacity upon volume creation. Volume creation will be slower when thick-provisioning is enabled.": "通过启用 thick-provisioning，卷将在创建卷时分配所需容量。当启用 thick-provisioning 时，创建卷的速度会减慢。",
  "{{resource}} details": "{{resource}}详情",
  "Kind": "种类（Kind）",
  "Labels": "标签",
  "Last updated": "最后更新",
  "Storage Systems": "存储系统",
  "Used capacity": "使用的容量",
  "Storage status represents the health status of Openshift Data Foundation's StorageCluster.": "存储状态代表 Openshift Data Foundation 的存储集群的健康状态。",
  "Health": "健康",
  "Openshift Data Foundation": "Openshift Data Foundation",
  "Standard": "Standard",
  "Data will be consumed by a Multi-cloud object gateway, deduped, compressed, and encrypted. The encrypted chunks would be saved on the selected BackingStores. Best used when the applications would always use the OpenShift Data Foundation endpoints to access the data.": "数据将被多云对象网关、dedupe、压缩和加密功能使用。加密的块将保存在所选后备存储中。最适合于在应用程序始终使用 OpenShift Data Foundation 端点访问数据时使用。",
  "Data is stored on the NamespaceStores without performing de-duplication, compression, or encryption. BucketClasses of namespace type allow connecting to existing data and serving from them. These are best used for existing data or when other applications (and cloud-native services) need to access the data from outside OpenShift Container Storage.": "数据存储在没有进行 dedupe、压缩、加密的命名空间中。命名空间类型的存储桶允许连接到现有数据并从中服务。最适用于现有数据，或其他应用程序（及原生云服务）需要从 OpenShift Container Storage 外部访问数据。",
  "Single NamespaceStore": "单一命名空间存储",
  "Multi NamespaceStores": "多命名空间存储",
  "The namespace bucket will serve reads from several selected backing stores, creating a virtual namespace on top of them and will write to one of those as its chosen write target": "命名空间存储桶将提供几个所选后备存储的读取服务，在其之上创建虚拟命名空间，并将写入其中之一作为它的选择的写入目标",
  "Cache NamespaceStore": "缓存命名空间存储",
  "The caching bucket will serve data from a large raw data out of a local caching tiering.": "缓存存储桶将提供来自本地缓存层的大型原始数据的数据。",
  "Create storage class": "创建存储类",
  "Create local volume set": "创建本地卷集",
  "Logical used capacity per account": "每个帐户的逻辑使用容量",
  "Egress Per Provider": "每个供应商的 Egress",
  "I/O Operations count": "I/O 操作数",
  "The infrastructure StorageClass used by OpenShift Container Storage to write its data and metadata.": "OpenShift Container Storage 用来写入其数据和元数据的基础架构存储类。",
  "Infrastructure StorageClass created by Local Storage Operator and used by OpenShift Container Storage to write its data and metadata.": "由 Local Storage Operator 创建的基础架构存储类，、OpenShift Container Storage 使用它来写入其数据和元数据的。",
  "The amount of capacity that would be dynamically allocated on the infrastructure StorageClass.": "在基础架构存储类中动态分配的容量。",
  "If you wish to use the Arbiter stretch cluster, a minimum of 4 nodes (2 different zones, 2 nodes per zone) and 1 additional zone with 1 node is required. All nodes must be pre-labeled with zones in order to be validated on cluster creation.": "如果您希望使用 Arbiter 扩展集群，则需要至少有 4 个节点（2 个不同的区、每个区有 2 个节点），以及一个额外的带有一个节点的区。所有节点都必须预先标记区以便在创建集群时进行验证。",
  "Selected nodes are based on the StorageClass <1>{{scName}}</1> and with a recommended requirement of 14 CPU and 34 GiB RAM per node.": "所选节点基于存储类 <1>{{scName}}</1>，推荐每个节点需要 14 个 CPU 和 34 GiB RAM。",
  "Selected nodes are based on the StorageClass <1>{{scName}}</1> and fulfill the stretch cluster requirements with a recommended requirement of 14 CPU and 34 GiB RAM per node.": "所选节点基于存储类 <1>{{scName}}</1>，为了实现集群的扩展，推荐每个节点需要 14 个 CPU 和 34 GiB RAM。",
  "Storage": "存储",
  "Disks": "磁盘",
  "Backing Store": "后端存储",
  "Bucket Class": "存储桶类",
  "Namespace Store": "命名空间存储",
  "Loading...": "正在载入...",
  "Pool {{name}} creation in progress": "池 {{name}} 创建正在进行中",
  "Pool {{name}} was successfully created": "池 {{name}} 成功创建",
  "An error occurred. Pool {{name}} was not created": "出现错误，池 {{name}} 没有创建",
  "Pool {{name}} creation timed out. Please check if odf operator and rook operator are running": "池 {{name}} 创建超时。请检查 ocs operator 和 rook operator 是否在运行",
  "The creation of a StorageCluster is still in progress or has failed. Try again after the StorageCuster is ready to use.": "创建存储集群仍在进行中，或者已经失败。请在存储集群就绪后再试。",
  "Pool management tasks are not supported for default pool and OpenShift Container Storage's external mode.": "默认池和 OpenShift Container Storage 的外部模式不支持池管理任务。",
  "Pool {{name}} was created with errors.": "池 {{name}} 已创建但有错误。",
  "Delete": "删除",
  "StorageClasses": "存储类",
  "hr": "小时",
  "min": "分钟",
  "A minimal cluster deployment will be performed.": "将会执行最小的集群部署。",
  "The selected nodes do not match OpenShift Container Storage's StorageCluster requirement of an aggregated 30 CPUs and 72 GiB of RAM. If the selection cannot be modified a minimal cluster will be deployed.": "所选节点与聚合的 30 个 CPU 和 72 GiB 内存的 OpenShift Container Storage 存储集群要求不匹配。如果无法修改选择，将部署最小集群。",
  "Back to nodes selection": "返回节点选择",
  "Select a StorageClass to continue": "选择一个存储类继续",
  "This is a required field. The StorageClass will be used to request storage from the underlying infrastructure to create the backing PersistentVolumes that will be used to provide the OpenShift Container Storage service.": "这是必需的字段。存储类将用于从底层基础架构请求存储，以创建用于提供 OpenShift Container Storage 服务的后端持久性卷。",
  "Create new StorageClass": "创建新存储类",
  "This is a required field. The StorageClass will be used to request storage from the underlying infrastructure to create the backing persistent volumes that will be used to provide the OpenShift Container Storage service.": "这是必需的字段。存储类将用于从底层基础架构请求存储，以创建用于提供 OpenShift Container Storage 服务的后端持久性卷。",
  "All required fields are not set": "所有必填字段没有设置",
  "In order to create the StorageCluster you must set the StorageClass, select at least 3 nodes (preferably in 3 different zones) and meet the minimum or recommended requirement": "要创建存储集群，您必须设置存储类选择至少 3 个节点（最好是在 3 个不同的区），并满足最低或推荐要求",
  "The StorageCluster requires a minimum of 3 nodes for the initial deployment. Please choose a different StorageClass or go to create a new LocalVolumeSet that matches the minimum node requirement.": "存储集群的初始部署最少需要 3 个节点。请选择不同的存储类，或创建一个符合最低节点要求的本地卷集。",
  "Create new volume set instance": "创建新卷集实例",
  "Select at least 1 encryption level or disable encryption.": "至少选择 1 个加密级别或禁用加密。",
  "Fill out the details in order to connect to key management system": "填写详情以便连接到密钥管理系统",
  "This is a required field.": "这是必添字段。",
  "Both public and cluster network attachment definition cannot be empty": "公共和集群网络附加定义不能为空",
  "A public or cluster network attachment definition must be selected to use Multus.": "必须选择一个公共或集群网络附加定义来使用 Multus。",
  "The number of selected zones is less than the minimum requirement of 3. If not modified a host-based failure domain deployment will be enforced.": "所选区的数量小于最低要求的 3 个。如果没有修改，一个基于主机的故障域部署将被强制实施。",
  "When the nodes in the selected StorageClass are spread across fewer than 3 availability zones, the StorageCluster will be deployed with the host based failure domain.": "当所选存储类中的节点分散于少于 3 个可用区时，存储集群会根据故障域部署到相应的主机。",
  "Cluster-Wide and StorageClass": "集群范围和存储类",
  "Cluster-Wide": "集群范围",
  "Select at least 2 Backing Store resources": "选择至少 2 个后端存储资源",
  "Select at least 1 Backing Store resource": "选择至少 1 个后端存储资源",
  "x {{replica}} replicas = {{osdSize, number}} TiB": "x {{replica}} 副本 = {{osdSize, number}} TiB",
  "SmallScale": "SmallScale",
  "0.5 TiB": "0.5 TiB",
  "2 TiB": "2 TiB",
  "LargeScale": "LargeScale",
  "4 TiB": "4 TiB",
  "{{osdSize, number}} TiB": "{{osdSize, number}} TiB",
  "Help": "帮助"
}